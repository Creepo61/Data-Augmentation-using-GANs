{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Data Augmentation using GANs </center></h1>\n",
    "\n",
    "In this notebook we will introduce the <a href=\"https://arxiv.org/abs/1611.07004\" target=\"_blank\">Pix2Pix</a> and the <a href=\"https://arxiv.org/abs/1703.10593\" target=\"_blank\">CycleGAN</a> training process.\n",
    "training process on data generated by the CARLA simulator. The purpose of our work was to provide a proof-of-concept architecture for a style transfer to perform data augmentation in terms of weather conditions for traffic signs. We used the <a href=\"https://carla.org/\" target=\"_blank\">carla simulator</a> to collect training examples.\n",
    " \n",
    "In the repository you will find a smaller dataset for demonstration purposes, you can upload your own dataset with two arbitrary domains you want to translate for training purposes, but make sure the dimensions fit, and use numpy's (.npz) format. Our solution is based on the tensorflow library, so make sure you have it installed.\n",
    "\n",
    "<div>\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"pictures/5best_rainy_dev.png\" alt=\"Bildbeschreibung\"  width=\"500\">\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "We will only give a brief introduction to the topics, so if you are not familiar with GANs, cGANs, Pix2Pix and CycleGAN, you are strongly advised to read the linked papers.\n",
    "\n",
    "## Core idear of GANs\n",
    "<a href=\"https://arxiv.org/abs/1406.2661\" target=\"_blank\">Generative Adversarial Networks</a> were introduced in 2014 by Goodfellow at el. They are considered to be a semi-supervised learning technique, where two competing networks play a minmax game. While the generator tries to fool the discriminator by generating fake data, the discriminator tries to correctly classify the data as either real or fake. Obviously, minimising the loss of the generator leads to maximising the loss of the discriminator, and vice versa.\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style=\"font-size: xx-small;\">\n",
    "  <img src=\"pictures/gan_architektur.png\" alt=\"Bildbeschreibung\"  width=\"500\">\n",
    "  <figcaption>Source: https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml/</figcaption>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pix2Pix\n",
    "\n",
    "Pix2Pix networks are an extension of conditional GANs (cGANs for short), introduced in 2016 by Isola et al. By requiring the data to fulfil certain conditions, the network is able to perform an image-to-image transition from one domain to another. Isola et al. proposed $L_{1}$ regularisation to ensure that the data generated by the generator is closer to the desired target domain. The results are astonishing:\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style=\"font-size: xx-small;\">\n",
    "  <img src=\"pictures/pix2pix_examples.jpg\" alt=\"Bildbeschreibung\"  width=\"600\">\n",
    "  <figcaption>Source: https://phillipi.github.io/pix2pix/</figcaption>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN\n",
    "\n",
    "CycleGAN networks overcome the biggest limitation of Pix2Pix networks, which requires paired data, and were introduced in 2017 by Zhu et al.\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style= \"font-size: xx-small;\" >\n",
    "  <img src=\"pictures/paired_unpaired.png\" alt=\"Bildbeschreibung\"  width=\"400\">\n",
    "  <figcaption>Source: https://towardsdatascience.com/cyclegan-how-machine-learning-learns </br>-unpaired-image-to-image-translation-3fa8d9a6aa1d</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "By introducing a second regularization with cycle consistency loss, image-to-image translation is even possible when the images are not paired.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style= \"font-size: xx-small;\" >\n",
    "  <img src=\"pictures/forbackcycle.png\" alt=\"Bildbeschreibung\"  width=\"700\">\n",
    "  <figcaption>Source: https://ethanyanjiali.medium.com/gender-swap- </br>and-cyclegan-in-tensorflow-2-0-359fe74ab7ff</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "As you can see in the figure, we require that the reconstruction of our image should be consistent. A heuristic approach is to say that if you have a translator for English and Portuguese than you would like that translation to yield the same result.\n",
    "</br> </br>\n",
    "<center> Mathematics  &rarr; Matemática </br>\n",
    "         Matemática  &rarr; Mathematics </center>\n",
    "\n",
    "So we call a translation consistent when we apply a retranslation and end up with the same result.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style= \"font-size: xx-small;\" >\n",
    "  <img src=\"pictures/cycleloss.png\" alt=\"Bildbeschreibung\"  width=\"700\">\n",
    "  <figcaption>Source: https://ethanyanjiali.medium.com/gender-swap- </br>and-cyclegan-in-tensorflow-2-0-359fe74ab7ff</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "Here are some examples from the CycleGAN paper:\n",
    "</br> </br>\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style= \"font-size: xx-small;\" >\n",
    "  <img src=\"pictures/cyclegan_examples.jpg\" alt=\"Bildbeschreibung\"  width=\"700\">\n",
    "  <figcaption>Source: https://junyanz.github.io/CycleGAN/</figcaption>\n",
    "</figure>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import the dataset. The dataset consists of 60 default and 60 rain images. The dimensions are 255 pixels in height and 255 pixels in length. The values are normalized to [-1,1], which is a standard procedure in image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset_small.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X1, X2]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m###real data\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_small.npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m dataA, dataB \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marr_0\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marr_1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded: \u001b[39m\u001b[38;5;124m'\u001b[39m, dataA\u001b[38;5;241m.\u001b[39mshape, dataB\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\burak.altin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset_small.npz'"
     ]
    }
   ],
   "source": [
    "def load_dataset(filename):\n",
    "    # load the compressed arrays\n",
    "    data = load(filename)\n",
    "    # unpack the arrays\n",
    "    X1, X2 = data['arr_0'], data['arr_1']\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X1 = (X1 - 127.5) / 127.5\n",
    "    X2 = (X2 - 127.5) / 127.5\n",
    "    return [X1, X2]\n",
    "\n",
    "###real data\n",
    "data = load(\"dataset_small.npz\")\n",
    "dataA, dataB = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', dataA.shape, dataB.shape)\n",
    "###normalized data\n",
    "dataset = load_dataset(\"dataset_small.npz\")\n",
    "image_shape = dataset[0].shape[1:]\n",
    "height = dataA.shape[1] \n",
    "width = dataA.shape[2] // 5\n",
    "new_width = width * 5 \n",
    "\n",
    "# plot source images\n",
    "n_samples = 5\n",
    "random_index =  [randint(1,60) for _ in range(n_samples)]\n",
    "for idx, i in enumerate(random_index, 1):\n",
    "    plt.subplot(2, n_samples, idx)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(dataA[i].astype('uint8'), extent=[0, new_width, 0, height])\n",
    "\n",
    "# Plot target images\n",
    "for idx, i in enumerate(random_index, 1):\n",
    "    plt.subplot(2, n_samples, n_samples + idx)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(dataB[i].astype('uint8'), extent=[0, new_width, 0, height])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataset consists of 60 images of the Default domain and 60 images of the Rainy domain. Both have a resolution of $255 \\times 255$ and consist of three color channels R, G and B. \n",
    "\n",
    "For the purpose of this notebook we will call the Default Domain $X$ and the Rainy Domain $Y$. For training we will use $t \\in \\{0,1\\}$ as out labels. If the data was taken from the dataset, then $t = 1$, or is generated by a generator $G(x)$, then $t = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix and CycleGAN Networks\n",
    "\n",
    "This code defines a class `define` that encapsulates the architecture of both Pix2Pix and CycleGAN networks. The class is further divided into subclasses for discriminators (`discriminator`), generators (`generator`), and adversarial models (`adversarial_model`). This code provides a modular and reusable implementation of these architectures for image-to-image translation tasks. We will not provide a layer-by-layer description, but the models will be explained later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class define():\n",
    "    class discriminator:\n",
    "      def patchgan(image_shape):   \n",
    "          # weight initialization\n",
    "            init = RandomNormal(stddev=0.02)\n",
    "            # source image input\n",
    "            in_src_image = Input(shape=image_shape)\n",
    "            # target image input\n",
    "            # patchgan will take the binary loss\n",
    "            in_target_image = Input(shape=image_shape)\n",
    "            # concatenate images channel-wise\n",
    "            merged = Concatenate()([in_src_image, in_target_image])\n",
    "            d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # patch output\n",
    "            d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            patch_out = Activation('sigmoid')(d)\n",
    "            # define model\n",
    "            model = Model([in_src_image, in_target_image], patch_out)\n",
    "            # compile model\n",
    "            opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "            model.model_type = 'patchgan'\n",
    "            model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "            return model\n",
    "      \n",
    "      def lsgan(image_shape):\n",
    "          # weight initialization\n",
    "            init = RandomNormal(stddev=0.02)\n",
    "            # source image input\n",
    "            in_image = Input(shape=image_shape)\n",
    "            d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # patch output\n",
    "            # ls_gan not activates the output\n",
    "            patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            # define model\n",
    "            model = Model(in_image, patch_out)\n",
    "            # compile model\n",
    "            model.model_type = 'lsgan'\n",
    "            model.compile(loss='mse', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss_weights=[0.5])\n",
    "            return model\n",
    "    \n",
    "    class generator:\n",
    "      def __define_encoder_block__(layer_in, n_filters, batchnorm=True):\n",
    "          # weight initialization\n",
    "          init = RandomNormal(stddev=0.02)\n",
    "          # add downsampling layer\n",
    "          g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same',\n",
    "          kernel_initializer=init)(layer_in)\n",
    "          # conditionally add batch normalization\n",
    "          if batchnorm:\n",
    "              g = BatchNormalization()(g, training=True)\n",
    "          # leaky relu activation\n",
    "          g = LeakyReLU(alpha=0.2)(g)\n",
    "          return g\n",
    "      def __decoder_block__(layer_in, skip_in, n_filters, dropout=True):\n",
    "          # weight initialization\n",
    "          init = RandomNormal(stddev=0.02)\n",
    "          # add upsampling layer\n",
    "          g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same',\n",
    "          kernel_initializer=init)(layer_in)\n",
    "          # add batch normalization\n",
    "          g = BatchNormalization()(g, training=True)\n",
    "          # conditionally add dropout, 50% of the neurons will be deactivated\n",
    "          if dropout:\n",
    "              g = Dropout(0.5)(g, training=True)\n",
    "          # merge with skip connection\n",
    "          g = Concatenate()([g, skip_in])\n",
    "          # relu activation\n",
    "          g = Activation('relu')(g)\n",
    "          return g\n",
    "      def _resnet_block(n_filters, input_layer):\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # first layer convolutional layer\n",
    "        g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        # second convolutional layer\n",
    "        g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        # concatenate merge channel-wise with input layer\n",
    "        g = Concatenate()([g, input_layer])\n",
    "        return g\n",
    "      \n",
    "      def u_net(image_shape):\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # image input\n",
    "        in_image = Input(shape=image_shape)\n",
    "        # encoder model\n",
    "        e1 = define.generator.__define_encoder_block__(in_image, 64, batchnorm=False)\n",
    "        e2 = define.generator.__define_encoder_block__(e1, 128)\n",
    "        e3 = define.generator.__define_encoder_block__(e2, 256)\n",
    "        e4 = define.generator.__define_encoder_block__(e3, 512)\n",
    "        e5 = define.generator.__define_encoder_block__(e4, 512)\n",
    "        e6 = define.generator.__define_encoder_block__(e5, 512)\n",
    "        e7 = define.generator.__define_encoder_block__(e6, 512)\n",
    "        # bottleneck, no batch norm and relu\n",
    "        b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "        b = Activation('relu')(b)\n",
    "        # decoder model\n",
    "        d1 = define.generator.__decoder_block__(b, e7, 512)\n",
    "        d2 = define.generator.__decoder_block__(d1, e6, 512)\n",
    "        d3 = define.generator.__decoder_block__(d2, e5, 512)\n",
    "        d4 = define.generator.__decoder_block__(d3, e4, 512, dropout=False)\n",
    "        d5 = define.generator.__decoder_block__(d4, e3, 256, dropout=False)\n",
    "        d6 = define.generator.__decoder_block__(d5, e2, 128, dropout=False)\n",
    "        d7 = define.generator.__decoder_block__(d6, e1, 64, dropout=False)\n",
    "        # output\n",
    "        g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "        out_image = Activation('tanh')(g)\n",
    "        # define model\n",
    "        model = Model(in_image, out_image)\n",
    "        model.model_type = 'unet'\n",
    "        return model\n",
    "      def res_net(image_shape, n_resnet=9):\n",
    "          # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # image input\n",
    "        in_image = Input(shape=image_shape)\n",
    "\n",
    "        g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        for _ in range(n_resnet):\n",
    "          g = define.generator._resnet_block(256, g)\n",
    "        g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        out_image = Activation('tanh')(g)\n",
    "        model = Model(in_image, out_image)\n",
    "        model.model_type = \"resnet\"\n",
    "        return model\n",
    "    class adversarial_model:\n",
    "       def cycle_gan(g_model_1, d_model, g_model_2, image_shape, lambda_1 = 1, lambda_2 = 5, lambda_3 = 10):\n",
    "        #the adversarial model only trains the Generator,\n",
    "        #so the other models will be set to trainable = False\n",
    "        if d_model.model_type == \"lsgan\" :\n",
    "           dloss = 'mse'\n",
    "        elif d_model.model_type == \"patchgan\":\n",
    "           dloss = 'binary_crossentropy'\n",
    "        \n",
    "        g_model_1.trainable = True\n",
    "        d_model.trainable = False\n",
    "        g_model_2.trainable = False\n",
    "        input_gen = Input(shape=image_shape)\n",
    "        gen1_out = g_model_1(input_gen)\n",
    "        if d_model.model_type == \"patchgan\":\n",
    "          output_d = d_model([input_gen, gen1_out])\n",
    "        elif d_model.model_type == \"lsgan\":\n",
    "          output_d = d_model(gen1_out)\n",
    "        input_id = Input(shape = image_shape)\n",
    "        output_id = g_model_1(input_id)\n",
    "        output_f = g_model_2(gen1_out)\n",
    "        gen2_out = g_model_2(input_id)\n",
    "        output_b = g_model_1(gen2_out)\n",
    "        model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=[dloss, 'mae', 'mae', 'mae'], loss_weights=[lambda_1, lambda_2, lambda_3, lambda_3], optimizer=opt)\n",
    "        return model\n",
    "       #gan_model.train_on_batch(X_realX, [t_real, X_realY])\n",
    "\n",
    "       def pix2pix(g_model, d_model, image_shape, lambda_1 = 1, lambda_2 = 100):\n",
    "        # make weights in the discriminator not trainable,\n",
    "        # since Batchnormalization must not be \"learned\"\n",
    "        if d_model.model_type == \"patchgan\":\n",
    "           dloss = 'binary_crossentropy'\n",
    "        if d_model.model_type == \"lsgan\":\n",
    "           dloss = 'mse'\n",
    "        for layer in d_model.layers:\n",
    "            if not isinstance(layer, BatchNormalization):\n",
    "                layer.trainable = False\n",
    "        # define the source image\n",
    "        in_src = Input(shape=image_shape)\n",
    "        gen_out = g_model(in_src)\n",
    "        if d_model.model_type == \"patchgan\":\n",
    "          dis_out = d_model([in_src, gen_out])\n",
    "        elif d_model.model_type == \"lsgan\":\n",
    "          dis_out = d_model(gen_out)\n",
    "        # src image as input, generated image and classification output\n",
    "        model = Model(in_src, [dis_out, gen_out])\n",
    "        # compile model\n",
    "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=[dloss, 'mae'], optimizer=opt, loss_weights=[lambda_1,lambda_2])\n",
    "        return model\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picture Generation Utilities\n",
    "\n",
    "This code defines a utility class `generate_pictures` with two subclasses (`real` and `fake`) responsible for either selecting a real picture from the training set or generating fake pictures from the generator for training the Pix2Pix and CycleGAN models.\n",
    "\n",
    "By using separate methods for real and fake instance generation, we provide flexibility and clarity for users working with either Pix2Pix or CycleGAN models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate_pictures():\n",
    "    class real():\n",
    "        def pick(dataset, n_samples, patch_shape, type):\n",
    "            if type == 'single':\n",
    "                #choose random pictures inside the dataset\n",
    "                ix = randint(0, dataset.shape[0], n_samples)\n",
    "                X = dataset[ix]\n",
    "                y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "                return X, y\n",
    "            elif type == 'pair':\n",
    "                trainA, trainB = dataset\n",
    "                # choose random instances\n",
    "                ix = randint(0, trainA.shape[0], n_samples)\n",
    "                # retrieve selected images\n",
    "                X1, X2 = trainA[ix], trainB[ix]\n",
    "                # generate 'real' class labels (1)\n",
    "                y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "                return [X1, X2], y\n",
    "    class fake():\n",
    "        def predict(g_model, dataset, patch_shape):\n",
    "            # generate fake instance\n",
    "            X = g_model.predict(dataset)\n",
    "            # create 'fake' class labels (0)\n",
    "            y = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "            return X, y\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Visualization Utilities\n",
    "\n",
    "The `performance` class provides methods to visualize and compare the performance of Pix2Pix and CycleGAN models during training. `pix2pix_loss` and `cycle_gan_loss` show training losses, while `pix2pix_pictures` and `cycle_gan_pictures` show image translations at different training steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class performance():\n",
    "    def pix2pix_loss(d_loss1_list, d_loss2_list, g_loss_list):\n",
    "        steps = range(1, len(d_loss1_list) + 1)\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.title('Training Losses over time - Epoch %d' %max(steps))\n",
    "        plt.plot(steps, d_loss1_list, label='D Loss Real', color='blue')\n",
    "        plt.plot(steps, d_loss2_list, label='D Loss Fake', color='green')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(steps, g_loss_list, label='G Loss', color='orange')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    def pix2pix_pictures(step, g_model, dataset, n_samples=5):\n",
    "        [X_realA, X_realB], _ = generate_pictures.real.pick(dataset, n_samples,0, type = 'pair')\n",
    "        # generate a batch of fake samples\n",
    "        X_fakeB, _ = generate_pictures.fake.predict(g_model, X_realA, 1)\n",
    "        # scale all pixels from [-1,1] to [0,1]\n",
    "        X_realA = (X_realA + 1) / 2.0\n",
    "        X_realB = (X_realB + 1) / 2.0\n",
    "        X_fakeB = (X_fakeB + 1) / 2.0\n",
    "        # plot real source images\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, 1 + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_realA[i])\n",
    "        # plot generated target image\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, 1 + n_samples + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_fakeB[i])\n",
    "        # plot real target image\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_realB[i])\n",
    "        plt.figtext(0.05, 0.95, \"Epoch %d\" % step, fontsize=12, ha='center')\n",
    "        plt.show()\n",
    "            \n",
    "    def cycle_gan_pictures(step, g_model, trainX, n_samples=5):\n",
    "        # select a sample of input images\n",
    "        X_in, _ = generate_pictures.real.pick(trainX, n_samples, 0, type = 'single')\n",
    "        # generate translated images\n",
    "        X_out, _ = generate_pictures.fake.predict(g_model, X_in, 0)\n",
    "        # scale all pixels from [-1,1] to [0,1]\n",
    "        X_in = (X_in + 1) / 2.0\n",
    "        X_out = (X_out + 1) / 2.0\n",
    "        # plot real images\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(2, n_samples, 1 + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_in[i])\n",
    "        # plot translated image\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(2, n_samples, 1 + n_samples + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_out[i])\n",
    "        plt.figtext(0.05, 0.95, \"%d. Epoch\" % step, fontsize=12, ha='left')\n",
    "        plt.show()\n",
    "    def cycle_gan_loss(dG_loss1_list, dG_loss2_list , g_loss_list, f_loss_list, dF_loss1_list ,dF_loss2_list):\n",
    "        steps = range(1, len(dG_loss1_list) + 1)\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.title('Training Losses over time - Epoch %d' %max(steps))\n",
    "        plt.plot(steps, dG_loss1_list, label='D_G Loss Real', color='blue')\n",
    "        plt.plot(steps, dG_loss2_list, label='D_G Loss Fake', color='green')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.plot(steps, g_loss_list, label='G Loss', color='orange')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.subplot(2,2,3)\n",
    "        plt.plot(steps, dF_loss1_list, label='D_F Loss Real', color='blue')\n",
    "        plt.plot(steps, dF_loss2_list, label='D_F Loss Fake', color='green')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.subplot(2,2,4)\n",
    "        plt.plot(steps, f_loss_list, label='F Loss', color='orange')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training Utilities\n",
    "\n",
    "The `train_gan` class combines the training methods for the Pix2Pix and CycleGAN models. The `pix2pix` method trains a Pix2Pix model, updating the discriminator and generator losses. The `cycle_gan` method trains a CycleGAN model, updating discriminator, generator, and cycle consistency losses. Both methods provide visualizations of the training progress using the `performance' class. We will go into more detail in a moment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_gan():\n",
    "    def __update_image_pool__(pool, images, max_size=50):\n",
    "        selected = list()\n",
    "        for image in images:\n",
    "            if len(pool) < max_size:\n",
    "                # stock the pool\n",
    "                pool.append(image)\n",
    "                selected.append(image)\n",
    "            elif random() < 0.5:\n",
    "                # use image, but don't add it to the pool\n",
    "                selected.append(image)\n",
    "            else:\n",
    "                # replace an existing image and use replaced image\n",
    "                ix = randint(0, len(pool))\n",
    "                selected.append(pool[ix])\n",
    "                pool[ix] = image\n",
    "        return asarray(selected)\n",
    "    def pix2pix(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n",
    "        n_patch = d_model.output_shape[1]\n",
    "        d_loss1_list = []\n",
    "        d_loss2_list = []\n",
    "        g_loss_list = []\n",
    "        if(d_model.model_type == 'patchgan'):\n",
    "            for i in range(n_epochs):\n",
    "                # select a batch of real samples with label t = 1\n",
    "                [X_realX, X_realY], t_real = generate_pictures.real.pick(dataset, n_batch, n_patch, type = 'pair')\n",
    "                # generate a batch of fake samples with label t = 0\n",
    "                X_fakeY, t_fake = generate_pictures.fake.predict(g_model, X_realX, n_patch)\n",
    "                # update discriminator for real samples\n",
    "                d_loss1 = d_model.train_on_batch([X_realX, X_realY], t_real)\n",
    "                d_loss1_list.append(d_loss1)\n",
    "                # update discriminator for generated samples\n",
    "                d_loss2 = d_model.train_on_batch([X_realX, X_fakeY], t_fake)\n",
    "                d_loss2_list.append(d_loss2)\n",
    "                # update the generator\n",
    "                g_loss, _, _ = gan_model.train_on_batch(X_realX, [t_real, X_realY])\n",
    "                g_loss_list.append(g_loss)\n",
    "                # summarize performance\n",
    "                clear_output(wait=True)\n",
    "                performance.pix2pix_pictures(i, g_model, dataset, n_samples=5)\n",
    "                performance.pix2pix_loss(d_loss1_list, d_loss2_list,g_loss_list)\n",
    "        if(d_model.model_type == 'lsgan'):\n",
    "            for i in range(n_epochs):\n",
    "                # select a batch of real samples with label t = 1\n",
    "                [X_realX, X_realY], t_real = generate_pictures.real.pick(dataset, n_batch, n_patch, type = 'pair')\n",
    "                # generate a batch of fake samples with label t = 0\n",
    "                X_fakeY, t_fake = generate_pictures.fake.predict(g_model, X_realX, n_patch)\n",
    "                # update discriminator for real samples \n",
    "                d_loss1 = d_model.train_on_batch(X_realX, t_real)\n",
    "                d_loss1_list.append(d_loss1)\n",
    "                # update discriminator for generated samples\n",
    "                d_loss2 = d_model.train_on_batch(X_fakeY, t_fake)\n",
    "                d_loss2_list.append(d_loss2)\n",
    "                # update the generator\n",
    "                g_loss, _, _ = gan_model.train_on_batch(X_realX, [t_real, X_realY])\n",
    "                g_loss_list.append(g_loss)\n",
    "                # summarize performance\n",
    "                clear_output(wait=True)\n",
    "                performance.pix2pix_pictures(i, g_model, dataset, n_samples=5)\n",
    "                performance.pix2pix_loss(d_loss1_list, d_loss2_list,g_loss_list)\n",
    "   \n",
    "\n",
    "    def cycle_gan(d_model_X, d_model_Y, g_model_XtoY, f_model_YtoX, a_model_XtoY, a_model_YtoX, dataset, n_epochs = 100):\n",
    "        n_batch = 1\n",
    "        n_patch = d_model_X.output_shape[1]\n",
    "        trainX, trainY = dataset\n",
    "        poolA, poolB = list(), list()\n",
    "        dG_loss1_list = []\n",
    "        dG_loss2_list = []\n",
    "        g_loss_list = []\n",
    "        f_loss_list = []\n",
    "        dF_loss1_list = []\n",
    "        dF_loss2_list = []\n",
    "        if(d_model_X.model_type != d_model_Y.model_type):\n",
    "            raise ValueError(\"d_model_X and d_model_Y don't match\")\n",
    "        elif(g_model_XtoY.model_type != g_model_XtoY.model_type):\n",
    "            raise ValueError(\"g_model_XtoY and g_model_YtoY don't match\")\n",
    "        else:\n",
    "            if d_model_X.model_type ==  \"lsgan\" :\n",
    "                for i in range(n_epochs):\n",
    "                    # select a batch of real samples with label t = 1\n",
    "                    X_realX, t_realX= generate_pictures.real.pick(trainX, n_batch, n_patch, type = 'single')\n",
    "                    X_realY, t_realY = generate_pictures.real.pick(trainY, n_batch, n_patch, type = 'single')\n",
    "                    # generate a batch of fake samples with label t = 0\n",
    "                    X_fakeY, t_fakeY = generate_pictures.fake.predict(f_model_YtoX, X_realY, n_patch)\n",
    "                    X_fakeX, t_fakeX = generate_pictures.fake.predict(g_model_XtoY, X_realX, n_patch)\n",
    "                    # update fakes from pool for cycle consistency loss \n",
    "                    X_fakeX = train_gan.__update_image_pool__(poolA, X_fakeX)\n",
    "                    X_fakeY = train_gan.__update_image_pool__(poolB, X_fakeY)\n",
    "                    # update generator Y->X via adversarial and cycle loss\n",
    "                    g_loss2, _, _, _, _  = a_model_YtoX.train_on_batch([X_realY, X_realX], [t_realX, X_realX, X_realY, X_realX])\n",
    "                    f_loss_list.append(g_loss2)\n",
    "                    # update discriminator for X -> [real/fake]\n",
    "                    dX_loss1 = d_model_X.train_on_batch(X_realX, t_realX)\n",
    "                    dX_loss2 = d_model_X.train_on_batch(X_fakeX, t_fakeX)\n",
    "                    ## the losses will be stored in this list to plot them \n",
    "                    dG_loss1_list.append(dX_loss1)\n",
    "                    dG_loss2_list.append(dX_loss2)\n",
    "                    # update generator X->Y via adversarial and cycle loss\n",
    "                    g_loss1, _, _, _, _ = a_model_XtoY.train_on_batch([X_realX, X_realY], [t_realY, X_realY, X_realX, X_realY])\n",
    "                    g_loss_list.append(g_loss1)\n",
    "                    # update discriminator for Y -> [real/fake]\n",
    "                    dY_loss1 = d_model_Y.train_on_batch(X_realY, t_realY)\n",
    "                    dY_loss2 = d_model_Y.train_on_batch(X_fakeY, t_fakeY)\n",
    "                    dF_loss1_list.append(dY_loss1)\n",
    "                    dF_loss2_list.append(dY_loss2)\n",
    "                    # summarize performance\n",
    "                    clear_output(wait=True)\n",
    "                    performance.cycle_gan_pictures(i, g_model_XtoY, trainX)\n",
    "                    performance.cycle_gan_loss(dG_loss1_list, dG_loss2_list , g_loss_list, f_loss_list, dF_loss1_list ,dF_loss2_list)\n",
    "            elif d_model_X.model_type ==  \"patchgan\" :\n",
    "                for i in range(n_epochs):\n",
    "                    # select a batch of real samples with label t = 1\n",
    "                    [X_realX, X_realY], t_realX = generate_pictures.real.pick(dataset, n_batch, n_patch, type = 'pair')\n",
    "                    [Y_realX, Y_realY], t_realY = generate_pictures.real.pick(dataset, n_batch, n_patch, type = 'pair')\n",
    "                    # generate a batch of fake samples with label t = 0\n",
    "                    Y_fakeY, t_fakeY = generate_pictures.fake.predict(f_model_YtoX, Y_realY, n_patch)\n",
    "                    X_fakeX, t_fakeX = generate_pictures.fake.predict(g_model_XtoY, X_realX, n_patch)\n",
    "                    # update fakes from pool for cycle consistency loss \n",
    "                    X_fakeX = train_gan.__update_image_pool__(poolA, X_fakeX)\n",
    "                    Y_fakeY = train_gan.__update_image_pool__(poolB, Y_fakeY)\n",
    "                    # update generator Y->X via adversarial and cycle loss\n",
    "                    g_loss2, _, _, _, _  = a_model_YtoX.train_on_batch([Y_realY, X_realX], [t_realX, X_realX, Y_realY, X_realX])\n",
    "                    f_loss_list.append(g_loss2)\n",
    "                    # update discriminator for X -> [real/fake]\n",
    "                    dX_loss1 = d_model_X.train_on_batch([X_realX, X_realY], t_realX)\n",
    "                    dX_loss2 = d_model_X.train_on_batch([X_realX, Y_fakeY], t_fakeX)\n",
    "                    ## the losses will be stored in this list to plot them \n",
    "                    dG_loss1_list.append(dX_loss1)\n",
    "                    dG_loss2_list.append(dX_loss2)\n",
    "                    # update generator X->Y via adversarial and cycle loss\n",
    "                    g_loss1, _, _, _, _ = a_model_XtoY.train_on_batch([X_realX, Y_realY], [t_realY, Y_realY, X_realX, Y_realY])\n",
    "                    g_loss_list.append(g_loss1)\n",
    "                    # update discriminator for Y -> [real/fake]\n",
    "                    dY_loss1 = d_model_Y.train_on_batch([Y_realY, Y_realX], t_realY)\n",
    "                    dY_loss2 = d_model_Y.train_on_batch([Y_realX, Y_fakeY], t_fakeY)\n",
    "                    dF_loss1_list.append(dY_loss1)\n",
    "                    dF_loss2_list.append(dY_loss2)\n",
    "                    # summarize performance\n",
    "                    clear_output(wait=True)\n",
    "                    performance.cycle_gan_pictures(i, g_model_XtoY, trainX)\n",
    "                    performance.cycle_gan_loss(dG_loss1_list, dG_loss2_list , g_loss_list, f_loss_list, dF_loss1_list ,dF_loss2_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix Live Training\n",
    "\n",
    "In the following cells, the Pix2Pix network is trained live, and the training results, including loss plots and image translations, are displayed below. The training progress is visualized using the defined `performance' utilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator PatchGAN\n",
    "\n",
    "The <a href=\"https://arxiv.org/abs/1611.07004\" target=\"_blank\">PatchGan</a> classifier gives us a probability that an image is real or fake. To do this, the classifier divides the image into patches and classifies them separately.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "  <img src=\"pictures/patchgan.png\" alt=\"Bildbeschreibung\"  width=\"400\">\n",
    "  <figcaption style=\"font-size: xx-small;\"> Source: https://www.researchgate.net/publication/336431839_Multipath_Ghost_and_SideGrating_Lobe_Suppression</br>_Based_on_Stacked_Generative_Adversarial_Nets_in_MIMO_Through-Wall_Radar_</br>Imaging/figures?lo=1&utm_source=google&utm_medium=organic</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "We use the sigmoid cross entropy as the loss function and the mean of all patches as our PatchGAN loss.\n",
    "\n",
    "$$L_{\\text{D}} = -\\frac{\\lambda}{N} \\sum_{i} \\left( t_i\\cdot \\log(D(y_i)) + (1 - t_i) \\cdot \\log(1 - D(G(x_i))) \\right) $$\n",
    "\n",
    "In this equation, $\\lambda$ is a damping parameter. Since discriminators tend to learn faster than generators, it is common to damp the loss function. In the literature, the damping is set to 0.5, which is what we did. This helps to avoid mode collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_model = define.discriminator.patchgan(image_shape)\n",
    "print(d_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the discriminator gets two inputs, corresponding to a batch of fake images with label 0 and real images with label 1. The concatenation layer simply merges them into one tensor. We use 5 convolutional layers to extract the features and activate these patches with the sigmoid activation function. To avoid gradient problems, batch anomalization and a LeakReLU activation were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Unet\n",
    "The <a href=\"https://arxiv.org/pdf/1505.04597.pdf\" target=\"_blank\">Unet</a> generator aims to transform the data using encoding and decoding layers. The image is encoded until it reaches a bottleneck, at which point it is upsampled to increase the spatial dimensions.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "  <img src=\"pictures/unet.png\" alt=\"Bildbeschreibung\"  width=\"400\">\n",
    "  <figcaption style=\"font-size: xx-small;\"> Source: https://www.researchgate.net/publication/336431839_Multipath_Ghost_and_SideGrating_Lobe_Suppression_Based</br>_on_Stacked_Generative_Adversarial_Nets_in_MIMO_Through-Wall_Radar_Imaging/figures?</br>_lo=1&utm_source=google&utm_medium=organic</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = define.generator.u_net(image_shape)\n",
    "print(g_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, under the hood we have a CNN with 7 encoder and 7 decoder blocks. The encoder blocks reduce the spatial dimensions until the bottleneck is reached. The decoder reverses this process until the spatial dimensions of the images are fully restored. During this process, drop-outs are performed to add some randomness to the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting the models now leads to the adversarial model. The adversarial model trains the generator while the discriminator trains itself. Remember, our goal is to find the optimal generator, by fooling the discriminator, which means the ($t=1$)-case even if out picture is generated. We will do this by minimizing the loss of the generator, e.g. maximizing the discriminators loss.\n",
    "$$L_{adv} = -\\frac{\\lambda}{N} \\sum_{i} \\log(D(G(x_i)))$$\n",
    "$$\\text{with} \\space \\space L_{\\text{L1}} = \\frac{1}{NM} \\sum_{i,j} |G(x)_{i,j} - y_{i,j}|$$\n",
    "$$L_{G} =  L_{adv} + \\lambda_{2} L_{1}$$ \n",
    "\n",
    "$L_{G}$ just maximizes the discriminator loss and minimizes the generator loss.\n",
    "\n",
    "$L_1$ is the absolute pixel loss between the generated fake sample and the real one by each single pixel. \\lambda$, on the other hand, allows us to control how strongly the regularization should penalize a deviation from the target domain. For the purpose of this work, $\\lambda_{1} $ = 1 and $\\lambda_{2}$ = 100 were chosen to emphasize the style transfer. By choosing $\\lambda_{2} $ quite high, we penalize when the generated output deviates from the desired output. This is what makes pix2pix so powerful when it comes to image translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the adversarial model\n",
    "gan_model = define.adversarial_model.pix2pix(g_model, d_model, image_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pix2Pix\n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "  <img src=\"pictures/pix2pix_training.png\" alt=\"Bildbeschreibung\"  width=\"400\">\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "As you can see in the figure, the training process is very simple. Each iteration starts with two images, here one is real and the other is fake. The discriminator is a CNN that processes both images through the network, chunking the images into patches. All of them are activated by the sigmoid activation function. After the Discriminator has made its prediction, the Binary-Cross-Entropy-Loss, or more precisely the derived update formula, will adjust the weights of the Discriminator. For the generator, we will also use the binary cross entropy loss and the L1 regularization in the adversarial model.\n",
    "<p> The following function executes the training process, shows some predictions, and plots the errors over the epochs: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan.pix2pix(d_model, g_model, gan_model,dataset=dataset, n_epochs= 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN Live Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator ResNet\n",
    "\n",
    "For the $G$ and $F$ generators, <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\">ResNet</a> was used on a CNN model that translates the image. \n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "  <img src=\"pictures/residual_block.png\" alt=\"Bildbeschreibung\"  width=\"400\">\n",
    " <figcaption style=\"font-size: xx-small;\"> Source: https://en.wikipedia.org/wiki/Residual_neural_network</figcaption>\n",
    " </figure>\n",
    "</div>\n",
    "ResNet gives us the ability to train deeper networks by adding residual blocks to the network within them, the information is not transformed. Identity mapping propagates information from one layer to another by simply skipping layers. These layers are also called skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator: X -> Y\n",
    "## n_resnet is the number of resnet blocks\n",
    "g_model_XtoY = define.generator.res_net(image_shape, n_resnet= 9)\n",
    "# generator: Y -> X\n",
    "f_model_YtoX = define.generator.res_net(image_shape, n_resnet= 9)\n",
    "\n",
    "print(g_model_XtoY.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ResNet model consists of 14 hidden layers, 9 of which have skip connections. Instance normalization was used to make the training process more stable. The activation function between the layers is ReLU and the last activation is tanh to produce values between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator LSGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As proposed in the CycleGAN paper, we will use the LSGAN model as the discriminator for the generated images. LSGAN also uses the PatchGAN architecture, with the difference that it introduces the mean square error instead of the sigmoid cross entropy. We chose a 1-0 encoding scheme for real and fake images and optain:\n",
    "$$ L^{(D)}_{\\text{LSGAN}}(D, G) = \\frac{1}{N}\\sum_{i=1}^N(D(y) - 1)^2 + D(G(x))^2 $$\n",
    "$$ L^{(G)}_{\\text{LSGAN}}(D, G) = \\frac{1}{N}\\sum_{i=1}^N(D(G(x)) - 1)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator: X -> [real/fake]\n",
    "d_model_X = define.discriminator.lsgan(image_shape)\n",
    "# discriminator: Y -> [real/fake]\n",
    "d_model_Y = define.discriminator.lsgan(image_shape)\n",
    "print(d_model_Y.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSGAN model takes an similar approach for the discriminator. We have 9 hidden layers and instead of activating the function our model output goes directly into the Lossfunction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CycleGAN we will train two adversarial_models. Let $A_{G}$ and $A_{D}$ be the models, then tying everything together leads to\n",
    "$$ A_{G} = \\underset{G}{\\arg \\min} \\left[ \\lambda_{1} \\cdot L^{(G)}_{\\text{LSGAN}}(D, G) + \\lambda_{2} \\cdot L_{\\text{L1}}(G) + \\lambda_{3} \\cdot  L_{\\text{cycle}}(G,F)  \\right] $$\n",
    "$$ A_{F} = \\underset{F}{\\arg \\min} \\left[ \\lambda_{1} \\cdot L^{(F)}_{\\text{LSGAN}}(D, F) + \\lambda_{2} \\cdot L_{\\text{L1}}(F) + \\lambda_{3} \\cdot  L_{\\text{cycle}}(G,F)  \\right] $$\n",
    "$$\\text{with} \\space \\space L_{\\text{cycle}} = \\frac{1}{NM} \\sum_{i,j} \\left|F(G(x))_{i,j} - x_{i,j}\\right| + \\left|G(F(y))_{i,j} - y_{i,j}\\right| $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this approach is just an extension of the pix2pix model. We see that the $L_{cycle}$ approach is heavily influenced by the $L_{1}$ loss. Only now we try to make the reconstruction consistent. For this work we chose $\\lambda_{1} = 1$, $\\lambda_{2}$ = 5 and $\\lambda_{3}$ = 10. Choosing a high $\\lambda_{3}$ cycle consistency has a high impact on the loss function. With $\\lambda_{2}$ we can make sure that the structure of the image is preserved, as this is crucial for style transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversarial: X -> Y -> [real/fake, X]\n",
    "a_model_XtoY = define.adversarial_model.cycle_gan(g_model_XtoY, d_model_Y, f_model_YtoX, image_shape)\n",
    "# adversarial: Y -> X -> [real/fake, Y]\n",
    "a_model_YtoX = define.adversarial_model.cycle_gan(f_model_YtoX, d_model_X, g_model_XtoY, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "  <img src=\"pictures/cycle_gan_training.png\" alt=\"Bildbeschreibung\"  width=\"600\">\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some say a picture is worth a 1000 words. With respect to this flow chart, we would argue that. Training a CycleGAN network is very complicated, but we will try to go through the process. As we can see, the two networks are translated from $ X \\rightarrow Y $ and from $ Y \\rightarrow X$. The generators $G$ and $F$ generate fake images which are stored in pool A and B for the calculation of the cycle consistency loss. Instead of activating the output, the LSGAN loss function takes the output directly. The generator and discriminator weights are adjusted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan.cycle_gan(d_model_X, d_model_Y, g_model_XtoY, f_model_YtoX, a_model_XtoY, a_model_YtoX, dataset, n_epochs= 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with the models\n",
    "\n",
    "This is where the classes come in handy, because we know we can train the models with different discriminators and parameters. Just try it out.\n",
    "You can comment and uncomment the architecture you want, and by playing around with the lambda values you can control how important the regularization becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pix2Pix\n",
    "#g_model = define.generator.u_net(image_shape)\n",
    "g_model = define.generator.res_net(image_shape, n_resnet= 2)\n",
    "#d_model = define.discriminator.patchgan(image_shape)\n",
    "d_model = define.discriminator.lsgan(image_shape)\n",
    "lambda_1 = 1 #control the influence of the adversarial loss; default = 1\n",
    "lambda_2 = 100 #control the influence of the L_1 pix loss; default = 100\n",
    "n_epochs = 100\n",
    "gan_model = define.adversarial_model.pix2pix(g_model, d_model, image_shape,lambda_1,lambda_2)\n",
    "train_gan.pix2pix(d_model, g_model, gan_model,dataset=dataset, n_epochs= n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CycleGAN\n",
    "\n",
    "# generator: X -> Y\n",
    "## n_resnet is the number of resnet blocks\n",
    "g_model_XtoY = define.generator.res_net(image_shape, n_resnet= 5)\n",
    "# generator: Y -> X\n",
    "f_model_YtoX = define.generator.res_net(image_shape, n_resnet= 5)\n",
    "\n",
    "# generator: X -> Y\n",
    "## n_resnet is the number of resnet blocks\n",
    "#g_model_XtoY = define.generator.u_net(image_shape)\n",
    "# generator: Y -> X\n",
    "#f_model_YtoX = define.generator.u_net(image_shape)\n",
    "\n",
    "# discriminator: X -> [real/fake]\n",
    "d_model_X = define.discriminator.lsgan(image_shape)\n",
    "# discriminator: Y -> [real/fake]\n",
    "d_model_Y = define.discriminator.lsgan(image_shape)\n",
    "\n",
    "# discriminator: X -> [real/fake]\n",
    "#d_model_X = define.discriminator.patchgan(image_shape)\n",
    "# discriminator: Y -> [real/fake]\n",
    "#d_model_Y = define.discriminator.patchgan(image_shape)\n",
    "lambda_1 = 1 #control the influence of the adversarial loss; default = 1\n",
    "lambda_2 = 5 #control the influence of the L_1 pix loss; default = 5\n",
    "lambda_3 = 10 #control the influence of the cycle consistency loss; default = 10\n",
    "n_epochs = 100\n",
    "# adversarial: X -> Y -> [real/fake, X]\n",
    "a_model_XtoY = define.adversarial_model.cycle_gan(g_model_XtoY, d_model_Y, f_model_YtoX, image_shape,lambda_1,lambda_2, lambda_3)\n",
    "# adversarial: Y -> X -> [real/fake, Y]\n",
    "a_model_YtoX = define.adversarial_model.cycle_gan(f_model_YtoX, d_model_X, g_model_XtoY, image_shape,lambda_1,lambda_2, lambda_3)\n",
    "train_gan.cycle_gan(d_model_X, d_model_Y, g_model_XtoY, f_model_YtoX, a_model_XtoY, a_model_YtoX, dataset, n_epochs= n_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
