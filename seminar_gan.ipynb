{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Data Augmentation using GANs </center></h1>\n",
    "\n",
    "In this notebook we will introduce the <a href=\"https://arxiv.org/abs/1611.07004\" target=\"_blank\">Pix2Pix</a> and <a href=\"https://arxiv.org/abs/1703.10593\" target=\"_blank\">CycleGAN</a>\n",
    "training process on data created from carla simulator. The purpose of our work was a proof of concept architecture for a style transfer to perform data augmention in therms of wheater condition for traffic signs. We used <a href=\"https://carla.org/\" target=\"_blank\">carla simulator</a> to gather training samples.\n",
    " \n",
    "In the repository you will find a smaller dataset for demonstration purposes, you can upload your own dataset with two arbitrary domains you want to translate for training purposes, but make sure that the dimensions fit, and you use the (.npz)-format of numpy. Our solution is based on the tensorflow library so make sure you have installed it. \n",
    "\n",
    "<div>\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"pictures/5best_rainy_dev.png\" alt=\"Bildbeschreibung\"  width=\"500\">\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "We will provide just a small introduction into the topics, so reading the linked papers is strongly advised when you are not familiar with GANs, cGANs, Pix2Pix and CycleGAN. \n",
    "\n",
    "## Core idear of GANs\n",
    "<a href=\"https://arxiv.org/abs/1406.2661\" target=\"_blank\">Generative Adversial Networks</a> were introduced 2014 by Goodfellow at el. and are considered as a semi-supervised learning technique, were two competing networks are playing a minmax game. While the Generator tryes to fool the Discriminator, by generating fake data, the Discriminator tryes to classify the data correctly as either real or fake. Obviously minimizing the Generator Loss leads to maximizing the loss of the Discriminator and vice versa.\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style=\"font-size: xx-small;\">\n",
    "  <img src=\"pictures/gan_architektur.png\" alt=\"Bildbeschreibung\"  width=\"500\">\n",
    "  <figcaption>Source: https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml/</figcaption>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pix2Pix\n",
    "\n",
    "Pix2Pix-Networks were introduced 2016 by Isola et al. and are extensions of conditional GANs (short cGANs). By requiring the data to fullfill certain conditions, the network is able to perform an image-to-image transition from a domain to an other. Isola et al. proposed the $L_{1}$-Regularization to ensure that the data generated by the Generator will be closer to the desired target domain. The output is astonishing:\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style=\"font-size: xx-small;\">\n",
    "  <img src=\"pictures/pix2pix_examples.jpg\" alt=\"Bildbeschreibung\"  width=\"600\">\n",
    "  <figcaption>Source: https://phillipi.github.io/pix2pix/</figcaption>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN\n",
    "\n",
    "CycleGAN-Netwroks were introduced 2017 by Zhu et al. and overcoming the biggest limitation of Pix2Pix-Networks, requiring paired data.\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style= \"font-size: xx-small;\" >\n",
    "  <img src=\"pictures/paired_unpaired.png\" alt=\"Bildbeschreibung\"  width=\"400\">\n",
    "  <figcaption>Source: https://towardsdatascience.com/cyclegan-how-machine-learning-learns </br>-unpaired-image-to-image-translation-3fa8d9a6aa1d</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "By introducing a second Regularization, with the cycle consistency loss, the image-to-image translation is even possible, when the pictures are not paired.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style= \"font-size: xx-small;\" >\n",
    "  <img src=\"pictures/forbackcycle.png\" alt=\"Bildbeschreibung\"  width=\"700\">\n",
    "  <figcaption>Source: https://ethanyanjiali.medium.com/gender-swap- </br>and-cyclegan-in-tensorflow-2-0-359fe74ab7ff</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "As can be seen in the figure, we demand that the reconstruction of our picture should be consistent. A heuristic approach is to say that, when you have a translator for English and Portuguese than you would like that translation yield to the same result.\n",
    "</br> </br>\n",
    "<center> Mathematics  &rarr; Matemática </br>\n",
    "         Matemática  &rarr; Mathematics </center>\n",
    "\n",
    "So we call a translation consistent, when applying a retranslation, we end up with the same result.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style= \"font-size: xx-small;\" >\n",
    "  <img src=\"pictures/cycleloss.png\" alt=\"Bildbeschreibung\"  width=\"700\">\n",
    "  <figcaption>Source: https://ethanyanjiali.medium.com/gender-swap- </br>and-cyclegan-in-tensorflow-2-0-359fe74ab7ff</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "Here are some examples from the CycleGAN-Paper:\n",
    "</br> </br>\n",
    "<div style=\"text-align: center;\">\n",
    "<figure style= \"font-size: xx-small;\" >\n",
    "  <img src=\"pictures/cyclegan_examples.jpg\" alt=\"Bildbeschreibung\"  width=\"700\">\n",
    "  <figcaption>Source: https://junyanz.github.io/CycleGAN/</figcaption>\n",
    "</figure>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "import inspect\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to import the dataset. The Dataset consists of 60 Default and 60 rainy pictures. The Dimensions are height 255 and length 255 Pixels. They values are normalized to [-1,1] which is a standard procedure in image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    # load the compressed arrays\n",
    "    data = load(filename)\n",
    "    # unpack the arrays\n",
    "    X1, X2 = data['arr_0'], data['arr_1']\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X1 = (X1 - 127.5) / 127.5\n",
    "    X2 = (X2 - 127.5) / 127.5\n",
    "    return [X1, X2]\n",
    "\n",
    "###real data\n",
    "data = load(\"dataset_small.npz\")\n",
    "dataA, dataB = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', dataA.shape, dataB.shape)\n",
    "###normalized data\n",
    "dataset = load_dataset(\"dataset_small.npz\")\n",
    "image_shape = dataset[0].shape[1:]\n",
    "height = dataA.shape[1] \n",
    "width = dataA.shape[2] // 5\n",
    "new_width = width * 5 \n",
    "\n",
    "# plot source images\n",
    "n_samples = 5\n",
    "random_index =  [randint(1,60) for _ in range(n_samples)]\n",
    "for idx, i in enumerate(random_index, 1):\n",
    "    pyplot.subplot(2, n_samples, idx)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(dataA[i].astype('uint8'), extent=[0, new_width, 0, height])\n",
    "\n",
    "# Plot target images\n",
    "for idx, i in enumerate(random_index, 1):\n",
    "    pyplot.subplot(2, n_samples, n_samples + idx)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(dataB[i].astype('uint8'), extent=[0, new_width, 0, height])\n",
    "\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the dataset consists of 60 Pictures of the Default Domain and 60 Pictures of Rainy Domain. Both of them have a Resolution of $255 \\times 255$ and consists of three Colorchannels R, G and B. \n",
    "\n",
    "For the purose of this notebook we will call the Default Domain $X$ and the Rainy Domain $Y$. For training we will use $t \\in \\{0,1\\}$ as out labels. If the data was taken from the dataset, then $t = 1$, or is generated by a Generator $G(x)$, then $t = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix and CycleGAN Networks\n",
    "\n",
    "This code defines a class `define` that encapsulates the architecture of both Pix2Pix and CycleGAN networks. The class is further divided into subclasses for discriminators (`discriminator`), generators (`generator`), and adversial models (`adversial_model`). This code provides a modular and reusable implementation of these architectures for image-to-image translation tasks. We will not provide a Layer-By-Layer-Description, but the models will be explained later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class define():\n",
    "    class discriminator:\n",
    "      def patchgan(image_shape):   \n",
    "          # weight initialization\n",
    "            init = RandomNormal(stddev=0.02)\n",
    "            # source image input\n",
    "            in_src_image = Input(shape=image_shape)\n",
    "            # target image input\n",
    "            # patchgan will take the binary loss\n",
    "            in_target_image = Input(shape=image_shape)\n",
    "            # concatenate images channel-wise\n",
    "            merged = Concatenate()([in_src_image, in_target_image])\n",
    "            d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # patch output\n",
    "            d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            patch_out = Activation('sigmoid')(d)\n",
    "            # define model\n",
    "            model = Model([in_src_image, in_target_image], patch_out)\n",
    "            # compile model\n",
    "            opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "            model.model_type = 'patchgan'\n",
    "            model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "            return model\n",
    "      \n",
    "      def lsgan(image_shape):\n",
    "          # weight initialization\n",
    "            init = RandomNormal(stddev=0.02)\n",
    "            # source image input\n",
    "            in_image = Input(shape=image_shape)\n",
    "            d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # patch output\n",
    "            # ls_gan not activates the output\n",
    "            patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            # define model\n",
    "            model = Model(in_image, patch_out)\n",
    "            # compile model\n",
    "            model.model_type = 'lsgan'\n",
    "            model.compile(loss='mse', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss_weights=[0.5])\n",
    "            return model\n",
    "    \n",
    "    class generator:\n",
    "      def __define_encoder_block__(layer_in, n_filters, batchnorm=True):\n",
    "          # weight initialization\n",
    "          init = RandomNormal(stddev=0.02)\n",
    "          # add downsampling layer\n",
    "          g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same',\n",
    "          kernel_initializer=init)(layer_in)\n",
    "          # conditionally add batch normalization\n",
    "          if batchnorm:\n",
    "              g = BatchNormalization()(g, training=True)\n",
    "          # leaky relu activation\n",
    "          g = LeakyReLU(alpha=0.2)(g)\n",
    "          return g\n",
    "      def __decoder_block__(layer_in, skip_in, n_filters, dropout=True):\n",
    "          # weight initialization\n",
    "          init = RandomNormal(stddev=0.02)\n",
    "          # add upsampling layer\n",
    "          g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same',\n",
    "          kernel_initializer=init)(layer_in)\n",
    "          # add batch normalization\n",
    "          g = BatchNormalization()(g, training=True)\n",
    "          # conditionally add dropout, 50% of the neurons will be deactivated\n",
    "          if dropout:\n",
    "              g = Dropout(0.5)(g, training=True)\n",
    "          # merge with skip connection\n",
    "          g = Concatenate()([g, skip_in])\n",
    "          # relu activation\n",
    "          g = Activation('relu')(g)\n",
    "          return g\n",
    "      def _resnet_block(n_filters, input_layer):\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # first layer convolutional layer\n",
    "        g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        # second convolutional layer\n",
    "        g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        # concatenate merge channel-wise with input layer\n",
    "        g = Concatenate()([g, input_layer])\n",
    "        return g\n",
    "      \n",
    "      def u_net(image_shape):\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # image input\n",
    "        in_image = Input(shape=image_shape)\n",
    "        # encoder model\n",
    "        e1 = define.generator.__define_encoder_block__(in_image, 64, batchnorm=False)\n",
    "        e2 = define.generator.__define_encoder_block__(e1, 128)\n",
    "        e3 = define.generator.__define_encoder_block__(e2, 256)\n",
    "        e4 = define.generator.__define_encoder_block__(e3, 512)\n",
    "        e5 = define.generator.__define_encoder_block__(e4, 512)\n",
    "        e6 = define.generator.__define_encoder_block__(e5, 512)\n",
    "        e7 = define.generator.__define_encoder_block__(e6, 512)\n",
    "        # bottleneck, no batch norm and relu\n",
    "        b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "        b = Activation('relu')(b)\n",
    "        # decoder model\n",
    "        d1 = define.generator.__decoder_block__(b, e7, 512)\n",
    "        d2 = define.generator.__decoder_block__(d1, e6, 512)\n",
    "        d3 = define.generator.__decoder_block__(d2, e5, 512)\n",
    "        d4 = define.generator.__decoder_block__(d3, e4, 512, dropout=False)\n",
    "        d5 = define.generator.__decoder_block__(d4, e3, 256, dropout=False)\n",
    "        d6 = define.generator.__decoder_block__(d5, e2, 128, dropout=False)\n",
    "        d7 = define.generator.__decoder_block__(d6, e1, 64, dropout=False)\n",
    "        # output\n",
    "        g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "        out_image = Activation('tanh')(g)\n",
    "        # define model\n",
    "        model = Model(in_image, out_image)\n",
    "        model.model_type = 'unet'\n",
    "        return model\n",
    "      def res_net(image_shape, n_resnet=9):\n",
    "          # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # image input\n",
    "        in_image = Input(shape=image_shape)\n",
    "\n",
    "        g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        for _ in range(n_resnet):\n",
    "          g = define.generator._resnet_block(256, g)\n",
    "        g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        out_image = Activation('tanh')(g)\n",
    "        model = Model(in_image, out_image)\n",
    "        model.model_type = \"resnet\"\n",
    "        return model\n",
    "    class adversial_model:\n",
    "       def cycle_gan(g_model_1, d_model, g_model_2, image_shape, lambda_1 = 1, lambda_2 = 5, lambda_3 = 10):\n",
    "        #the adversial model only trains the Generator,\n",
    "        #so the other models will be set to trainable = False\n",
    "        if d_model.model_type == \"lsgan\" :\n",
    "           dloss = 'mse'\n",
    "        elif d_model.model_type == \"patchgan\":\n",
    "           dloss = 'binary_crossentropy'\n",
    "        \n",
    "        g_model_1.trainable = True\n",
    "        d_model.trainable = False\n",
    "        g_model_2.trainable = False\n",
    "        input_gen = Input(shape=image_shape)\n",
    "        gen1_out = g_model_1(input_gen)\n",
    "        if d_model.model_type == \"patchgan\":\n",
    "          output_d = d_model([input_gen, gen1_out])\n",
    "        elif d_model.model_type == \"lsgan\":\n",
    "          output_d = d_model(gen1_out)\n",
    "        input_id = Input(shape = image_shape)\n",
    "        output_id = g_model_1(input_id)\n",
    "        output_f = g_model_2(gen1_out)\n",
    "        gen2_out = g_model_2(input_id)\n",
    "        output_b = g_model_1(gen2_out)\n",
    "        model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=[dloss, 'mae', 'mae', 'mae'], loss_weights=[lambda_1, lambda_2, lambda_3, lambda_3], optimizer=opt)\n",
    "        return model\n",
    "       #gan_model.train_on_batch(X_realX, [t_real, X_realY])\n",
    "\n",
    "       def pix2pix(g_model, d_model, image_shape, lambda_1 = 1, lambda_2 = 100):\n",
    "        # make weights in the discriminator not trainable,\n",
    "        # since Batchnormalization must not be \"learned\"\n",
    "        if d_model.model_type == \"patchgan\":\n",
    "           dloss = 'binary_crossentropy'\n",
    "        if d_model.model_type == \"lsgan\":\n",
    "           dloss = 'mse'\n",
    "        for layer in d_model.layers:\n",
    "            if not isinstance(layer, BatchNormalization):\n",
    "                layer.trainable = False\n",
    "        # define the source image\n",
    "        in_src = Input(shape=image_shape)\n",
    "        gen_out = g_model(in_src)\n",
    "        if d_model.model_type == \"patchgan\":\n",
    "          dis_out = d_model([in_src, gen_out])\n",
    "        elif d_model.model_type == \"lsgan\":\n",
    "          dis_out = d_model(gen_out)\n",
    "        # src image as input, generated image and classification output\n",
    "        model = Model(in_src, [dis_out, gen_out])\n",
    "        # compile model\n",
    "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=[dloss, 'mae'], optimizer=opt, loss_weights=[lambda_1,lambda_2])\n",
    "        return model\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picture Generation Utilities\n",
    "\n",
    "This code defines a utility class `generate_pictures` with two sub-classes (`real` and `fake`) responsible for either chosing a real picture from the trainingset or generate fake ones by the generator for training the Pix2Pix and CycleGAN models.\n",
    "\n",
    "By using separate methods for real and fake instance generation, we provide flexibility and clarity for users working with either Pix2Pix or CycleGAN models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate_pictures():\n",
    "    class real():\n",
    "        def pick(dataset, n_samples, patch_shape, type):\n",
    "            if type == 'single':\n",
    "                #choose random pictures inside the dataset\n",
    "                ix = randint(0, dataset.shape[0], n_samples)\n",
    "                X = dataset[ix]\n",
    "                y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "                return X, y\n",
    "            elif type == 'pair':\n",
    "                trainA, trainB = dataset\n",
    "                # choose random instances\n",
    "                ix = randint(0, trainA.shape[0], n_samples)\n",
    "                # retrieve selected images\n",
    "                X1, X2 = trainA[ix], trainB[ix]\n",
    "                # generate 'real' class labels (1)\n",
    "                y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "                return [X1, X2], y\n",
    "    class fake():\n",
    "        def predict(g_model, dataset, patch_shape):\n",
    "            # generate fake instance\n",
    "            X = g_model.predict(dataset)\n",
    "            # create 'fake' class labels (0)\n",
    "            y = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "            return X, y\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Visualization Utilities\n",
    "\n",
    "The `performance` class provides methods to visualize and compare Pix2Pix and CycleGAN model performance during training. `pix2pix_loss` and `cycle_gan_loss` plot training losses, while `pix2pix_pictures` and `cycle_gan_pictures` display image translations at different training steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class performance():\n",
    "    def pix2pix_loss(d_loss1_list, d_loss2_list, g_loss_list):\n",
    "        steps = range(1, len(d_loss1_list) + 1)\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.title('Training Losses over time - Epoch %d' %max(steps))\n",
    "        plt.plot(steps, d_loss1_list, label='D Loss Real', color='blue')\n",
    "        plt.plot(steps, d_loss2_list, label='D Loss Fake', color='green')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(steps, g_loss_list, label='G Loss', color='orange')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    def pix2pix_pictures(step, g_model, dataset, n_samples=5):\n",
    "        [X_realA, X_realB], _ = generate_pictures.real.pick(dataset, n_samples,0, type = 'pair')\n",
    "        # generate a batch of fake samples\n",
    "        X_fakeB, _ = generate_pictures.fake.predict(g_model, X_realA, 1)\n",
    "        # scale all pixels from [-1,1] to [0,1]\n",
    "        X_realA = (X_realA + 1) / 2.0\n",
    "        X_realB = (X_realB + 1) / 2.0\n",
    "        X_fakeB = (X_fakeB + 1) / 2.0\n",
    "        # plot real source images\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, 1 + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_realA[i])\n",
    "        # plot generated target image\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, 1 + n_samples + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_fakeB[i])\n",
    "        # plot real target image\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_realB[i])\n",
    "        plt.figtext(0.05, 0.95, \"Epoch %d\" % step, fontsize=12, ha='center')\n",
    "        plt.show()\n",
    "            \n",
    "    def cycle_gan_pictures(step, g_model, trainX, n_samples=5):\n",
    "        # select a sample of input images\n",
    "        X_in, _ = generate_pictures.real.pick(trainX, n_samples, 0, type = 'single')\n",
    "        # generate translated images\n",
    "        X_out, _ = generate_pictures.fake.predict(g_model, X_in, 0)\n",
    "        # scale all pixels from [-1,1] to [0,1]\n",
    "        X_in = (X_in + 1) / 2.0\n",
    "        X_out = (X_out + 1) / 2.0\n",
    "        # plot real images\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(2, n_samples, 1 + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_in[i])\n",
    "        # plot translated image\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(2, n_samples, 1 + n_samples + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_out[i])\n",
    "        plt.figtext(0.05, 0.95, \"%d. Epoch\" % step, fontsize=12, ha='left')\n",
    "        plt.show()\n",
    "    def cycle_gan_loss(dG_loss1_list, dG_loss2_list , g_loss_list, f_loss_list, dF_loss1_list ,dF_loss2_list):\n",
    "        steps = range(1, len(dG_loss1_list) + 1)\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.title('Training Losses over time - Epoch %d' %max(steps))\n",
    "        plt.plot(steps, dG_loss1_list, label='D_G Loss Real', color='blue')\n",
    "        plt.plot(steps, dG_loss2_list, label='D_G Loss Fake', color='green')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.plot(steps, g_loss_list, label='G Loss', color='orange')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.subplot(2,2,3)\n",
    "        plt.plot(steps, dF_loss1_list, label='D_F Loss Real', color='blue')\n",
    "        plt.plot(steps, dF_loss2_list, label='D_F Loss Fake', color='green')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.subplot(2,2,4)\n",
    "        plt.plot(steps, f_loss_list, label='F Loss', color='orange')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training Utilities\n",
    "\n",
    "The `train_gan` class puts together the training methods for the Pix2Pix and CycleGAN models. The `pix2pix` method trains a Pix2Pix model, updating discriminator and generator losses. The `cycle_gan` method trains a CycleGAN model, updating discriminator, generator, and cycle consistency losses. Both methods provide visualizations of the training progress using the `performance` class. We will go more in detail in a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_gan():\n",
    "    def __update_image_pool__(pool, images, max_size=50):\n",
    "        selected = list()\n",
    "        for image in images:\n",
    "            if len(pool) < max_size:\n",
    "                # stock the pool\n",
    "                pool.append(image)\n",
    "                selected.append(image)\n",
    "            elif random() < 0.5:\n",
    "                # use image, but don't add it to the pool\n",
    "                selected.append(image)\n",
    "            else:\n",
    "                # replace an existing image and use replaced image\n",
    "                ix = randint(0, len(pool))\n",
    "                selected.append(pool[ix])\n",
    "                pool[ix] = image\n",
    "        return asarray(selected)\n",
    "    def pix2pix(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n",
    "        n_patch = d_model.output_shape[1]\n",
    "        d_loss1_list = []\n",
    "        d_loss2_list = []\n",
    "        g_loss_list = []\n",
    "        if(d_model.model_type == 'patchgan'):\n",
    "            for i in range(n_epochs):\n",
    "                # select a batch of real samples with label t = 1\n",
    "                [X_realX, X_realY], t_real = generate_pictures.real.pick(dataset, n_batch, n_patch, type = 'pair')\n",
    "                # generate a batch of fake samples with label t = 0\n",
    "                X_fakeY, t_fake = generate_pictures.fake.predict(g_model, X_realX, n_patch)\n",
    "                # update discriminator for real samples\n",
    "                d_loss1 = d_model.train_on_batch([X_realX, X_realY], t_real)\n",
    "                d_loss1_list.append(d_loss1)\n",
    "                # update discriminator for generated samples\n",
    "                d_loss2 = d_model.train_on_batch([X_realX, X_fakeY], t_fake)\n",
    "                d_loss2_list.append(d_loss2)\n",
    "                # update the generator\n",
    "                g_loss, _, _ = gan_model.train_on_batch(X_realX, [t_real, X_realY])\n",
    "                g_loss_list.append(g_loss)\n",
    "                # summarize performance\n",
    "                clear_output(wait=True)\n",
    "                performance.pix2pix_pictures(i, g_model, dataset, n_samples=5)\n",
    "                performance.pix2pix_loss(d_loss1_list, d_loss2_list,g_loss_list)\n",
    "        if(d_model.model_type == 'lsgan'):\n",
    "            for i in range(n_epochs):\n",
    "                # select a batch of real samples with label t = 1\n",
    "                [X_realX, X_realY], t_real = generate_pictures.real.pick(dataset, n_batch, n_patch, type = 'pair')\n",
    "                # generate a batch of fake samples with label t = 0\n",
    "                X_fakeY, t_fake = generate_pictures.fake.predict(g_model, X_realX, n_patch)\n",
    "                # update discriminator for real samples\n",
    "                d_loss1 = d_model.train_on_batch(X_realX, t_real)\n",
    "                #d_loss1 = d_model.train_on_batch([X_realX, X_realY], t_real)\n",
    "                d_loss1_list.append(d_loss1)\n",
    "                # update discriminator for generated samples\n",
    "                d_loss2 = d_model.train_on_batch(X_fakeY, t_fake)\n",
    "                d_loss2_list.append(d_loss2)\n",
    "                # update the generator\n",
    "                g_loss, _, _ = gan_model.train_on_batch(X_realX, [t_real, X_realY])\n",
    "                g_loss_list.append(g_loss)\n",
    "                # summarize performance\n",
    "                clear_output(wait=True)\n",
    "                performance.pix2pix_pictures(i, g_model, dataset, n_samples=5)\n",
    "                performance.pix2pix_loss(d_loss1_list, d_loss2_list,g_loss_list)\n",
    "   \n",
    "\n",
    "    def cycle_gan(d_model_X, d_model_Y, g_model_XtoY, f_model_YtoX, a_model_XtoY, a_model_YtoX, dataset, n_epochs = 100):\n",
    "        n_batch = 1\n",
    "        n_patch = d_model_X.output_shape[1]\n",
    "        trainX, trainY = dataset\n",
    "        poolA, poolB = list(), list()\n",
    "        dG_loss1_list = []\n",
    "        dG_loss2_list = []\n",
    "        g_loss_list = []\n",
    "        f_loss_list = []\n",
    "        dF_loss1_list = []\n",
    "        dF_loss2_list = []\n",
    "        if(d_model_X.model_type != d_model_Y.model_type):\n",
    "            raise ValueError(\"d_model_X and d_model_Y don't match\")\n",
    "        elif(g_model_XtoY.model_type != g_model_XtoY.model_type):\n",
    "            raise ValueError(\"g_model_XtoY and g_model_YtoY don't match\")\n",
    "        else:\n",
    "            if d_model_X.model_type ==  \"lsgan\" :\n",
    "                for i in range(n_epochs):\n",
    "                    # select a batch of real samples with label t = 1\n",
    "                    X_realX, t_realX= generate_pictures.real.pick(trainX, n_batch, n_patch, type = 'single')\n",
    "                    #X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n",
    "                    X_realY, t_realY = generate_pictures.real.pick(trainY, n_batch, n_patch, type = 'single')\n",
    "                    #X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n",
    "                    # generate a batch of fake samples with label t = 0\n",
    "                    X_fakeY, t_fakeY = generate_pictures.fake.predict(f_model_YtoX, X_realY, n_patch)\n",
    "                    X_fakeX, t_fakeX = generate_pictures.fake.predict(g_model_XtoY, X_realX, n_patch)\n",
    "                    # update fakes from pool for cycle consistency loss \n",
    "                    X_fakeX = train_gan.__update_image_pool__(poolA, X_fakeX)\n",
    "                    X_fakeY = train_gan.__update_image_pool__(poolB, X_fakeY)\n",
    "                    # update generator Y->X via adversarial and cycle loss\n",
    "                    g_loss2, _, _, _, _  = a_model_YtoX.train_on_batch([X_realY, X_realX], [t_realX, X_realX, X_realY, X_realX])\n",
    "                    f_loss_list.append(g_loss2)\n",
    "                    # update discriminator for X -> [real/fake]\n",
    "                    dX_loss1 = d_model_X.train_on_batch(X_realX, t_realX)\n",
    "                    dX_loss2 = d_model_X.train_on_batch(X_fakeX, t_fakeX)\n",
    "                    ## the losses will be stored in this list to plot them \n",
    "                    dG_loss1_list.append(dX_loss1)\n",
    "                    dG_loss2_list.append(dX_loss2)\n",
    "                    # update generator X->Y via adversarial and cycle loss\n",
    "                    g_loss1, _, _, _, _ = a_model_XtoY.train_on_batch([X_realX, X_realY], [t_realY, X_realY, X_realX, X_realY])\n",
    "                    g_loss_list.append(g_loss1)\n",
    "                    # update discriminator for Y -> [real/fake]\n",
    "                    dY_loss1 = d_model_Y.train_on_batch(X_realY, t_realY)\n",
    "                    dY_loss2 = d_model_Y.train_on_batch(X_fakeY, t_fakeY)\n",
    "                    dF_loss1_list.append(dY_loss1)\n",
    "                    dF_loss2_list.append(dY_loss2)\n",
    "                    # summarize performance\n",
    "                    clear_output(wait=True)\n",
    "                    performance.cycle_gan_pictures(i, g_model_XtoY, trainX)\n",
    "                    performance.cycle_gan_loss(dG_loss1_list, dG_loss2_list , g_loss_list, f_loss_list, dF_loss1_list ,dF_loss2_list)\n",
    "            elif d_model_X.model_type ==  \"patchgan\" :\n",
    "                for i in range(n_epochs):\n",
    "                    # select a batch of real samples with label t = 1\n",
    "                    [X_realX, X_realY], t_realX = generate_pictures.real.pick(dataset, n_batch, n_patch, type = 'pair')\n",
    "                    [Y_realX, Y_realY], t_realY = generate_pictures.real.pick(dataset, n_batch, n_patch, type = 'pair')\n",
    "                    # generate a batch of fake samples with label t = 0\n",
    "                    Y_fakeY, t_fakeY = generate_pictures.fake.predict(f_model_YtoX, Y_realY, n_patch)\n",
    "                    X_fakeX, t_fakeX = generate_pictures.fake.predict(g_model_XtoY, X_realX, n_patch)\n",
    "                    # update fakes from pool for cycle consistency loss \n",
    "                    X_fakeX = train_gan.__update_image_pool__(poolA, X_fakeX)\n",
    "                    Y_fakeY = train_gan.__update_image_pool__(poolB, Y_fakeY)\n",
    "                    # update generator Y->X via adversarial and cycle loss\n",
    "                    g_loss2, _, _, _, _  = a_model_YtoX.train_on_batch([Y_realY, X_realX], [t_realX, X_realX, Y_realY, X_realX])\n",
    "                    f_loss_list.append(g_loss2)\n",
    "                    # update discriminator for X -> [real/fake]\n",
    "                    dX_loss1 = d_model_X.train_on_batch([X_realX, X_realY], t_realX)\n",
    "                    dX_loss2 = d_model_X.train_on_batch([X_realX, Y_fakeY], t_fakeX)\n",
    "                    ## the losses will be stored in this list to plot them \n",
    "                    dG_loss1_list.append(dX_loss1)\n",
    "                    dG_loss2_list.append(dX_loss2)\n",
    "                    # update generator X->Y via adversarial and cycle loss\n",
    "                    g_loss1, _, _, _, _ = a_model_XtoY.train_on_batch([X_realX, Y_realY], [t_realY, Y_realY, X_realX, Y_realY])\n",
    "                    g_loss_list.append(g_loss1)\n",
    "                    # update discriminator for Y -> [real/fake]\n",
    "                    dY_loss1 = d_model_Y.train_on_batch([Y_realY, Y_realX], t_realY)\n",
    "                    dY_loss2 = d_model_Y.train_on_batch([Y_realX, Y_fakeY], t_fakeY)\n",
    "                    dF_loss1_list.append(dY_loss1)\n",
    "                    dF_loss2_list.append(dY_loss2)\n",
    "                    # summarize performance\n",
    "                    clear_output(wait=True)\n",
    "                    performance.cycle_gan_pictures(i, g_model_XtoY, trainX)\n",
    "                    performance.cycle_gan_loss(dG_loss1_list, dG_loss2_list , g_loss_list, f_loss_list, dF_loss1_list ,dF_loss2_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix Live Training\n",
    "\n",
    "In the following cells, the Pix2Pix network will undergo live training, and the training results, including loss plots and image translations, will be displayed below. The training progress will be visualized using the defined `performance` utilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator PatchGAN\n",
    "\n",
    "The <a href=\"https://arxiv.org/abs/1611.07004\" target=\"_blank\">PatchGan</a>-Classifier gives us a probability that a image is real or fake. To do so, the classifier devides the picture into patches and classifies them seperatly. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "  <img src=\"pictures/patchgan.png\" alt=\"Bildbeschreibung\"  width=\"400\">\n",
    "  <figcaption style=\"font-size: xx-small;\"> Source: https://www.researchgate.net/publication/336431839_Multipath_Ghost_and_SideGrating_Lobe_Suppression</br>_Based_on_Stacked_Generative_Adversarial_Nets_in_MIMO_Through-Wall_Radar_</br>Imaging/figures?lo=1&utm_source=google&utm_medium=organic</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "We use the Sigmoider Crossentropy as a loss-function, and the mean of all patches as our PatchGAN-loss.\n",
    "\n",
    "$$L_{\\text{D}} = -\\frac{\\lambda}{N} \\sum_{i} \\left( t_i\\cdot \\log(D(y)) + (1 - t_i) \\cdot \\log(1 - D(G(x))) \\right) $$\n",
    "\n",
    "In this equation $\\lambda$ is a damping parameter. Since Discriminators tent to lern faster than the generators, it is usual to damp the loss-function. In the literature damping is chosen to be 0.5 and so we did. This helps to avoid a mode collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_model = define.discriminator.patchgan(image_shape)\n",
    "print(d_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Discriminator gets two inputs, corresponding to a batch of fake images with label 0 and real ones with label 1. The concateate Layer just merges them into a tensor. We use 5 convolutional layers to extract the features and activate those patches with the sigmoid activation funciton. To avoid gradiend problems batch noarmalization and a LeakReLU-Activation were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Unet\n",
    "The <a href=\"https://arxiv.org/pdf/1505.04597.pdf\" target=\"_blank\">Unet</a>-Generator aimes to transform the data by using encoding end decoding layers. The picture will be encoded until reaching a bottleneck and from that point on upsampling will be performed to enhance the spatial dimensions.\n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "  <img src=\"pictures/unet.png\" alt=\"Bildbeschreibung\"  width=\"400\">\n",
    "  <figcaption style=\"font-size: xx-small;\"> Source: https://www.researchgate.net/publication/336431839_Multipath_Ghost_and_SideGrating_Lobe_Suppression_Based</br>_on_Stacked_Generative_Adversarial_Nets_in_MIMO_Through-Wall_Radar_Imaging/figures?</br>_lo=1&utm_source=google&utm_medium=organic</figcaption>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = define.generator.u_net(image_shape)\n",
    "print(g_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see, under the hood we have a CNN with 7 Encoder and 7 Decoder blocks. The Encoder Blocks decrease the spatial dimensions until the bottleneck is reached. The decoder reverses this process until the spatial dimensions of the pictures are totally restored. While this process drop outs where performed to add some randomness into the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tying the models together leads to the adversial model. The adversial model trains the Generator, while the Discriminator trains itself. Don't forget, our target is to find the optimal Generator. We will do that by minimizing the Loss of the Generator.\n",
    "$$L_{G} =  -\\lambda_{1}L_{D} + \\lambda_{2} L_1$$\n",
    "$$\\text{with} \\space \\space L_{\\text{L1}} = \\frac{1}{N} \\sum_{i,j} |G(x)_{i,j} - y_{i,j}|$$\n",
    "\n",
    "-$L_{D}$ is just maximizing the Discriminator Loss and minimizes des Generatorloss.\n",
    "\n",
    "$L_1$ is the absolute pixelloss between the generated fake sample and the real one by every single pixel. $ \\lambda $ on the other hand allows us to controll how strong the regularization should punish a deviaten from the target domain. For the purpose of this work, $ \\lambda_{1} $ = 1 and $\\lambda_{2}$ = 100 were choosen to emphazise the style transfer. By choosing $ \\lambda_{2} $ quite high, we penalize when the generated output deviation from the desired output. This makes pix2pix so powerfull, when it comes to image translating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the adversial model\n",
    "gan_model = define.adversial_model.pix2pix(g_model, d_model, image_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pix2Pix\n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "  <img src=\"pictures/pix2pix_training.png\" alt=\"Bildbeschreibung\"  width=\"400\">\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "As can be seen in the figure, the trainingprocess is very straightforward. Each iteration starts with two pictures, here one of them is real and the other one is fake. The Discriminator is a CNN processing both images through the network, chunking the pictures in patches. All of them get activated by the Sigmoider Activation Function. After the Discriminator made his prediction, the Binary-Cross-Entropy-Loss, or to be more precise, the derivated update formula, will adjust the weights of the Discriminator. For the Generator, we will use also the Binary-Cross-Entropy-loss and the L1-Regularization in the adversial model.\n",
    "<p> The following function excecutes the training process,  showes some predictions and plots the errors over the epochs: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan.pix2pix(d_model, g_model, gan_model,dataset=dataset, n_epochs= 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN Live Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator ResNet\n",
    "\n",
    "For the Generator $G$ and $F$ <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\">ResNet</a> was used on a CNN model translating the picture. \n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "  <img src=\"pictures/residual_block.png\" alt=\"Bildbeschreibung\"  width=\"400\">\n",
    " <figcaption style=\"font-size: xx-small;\"> Source: https://en.wikipedia.org/wiki/Residual_neural_network</figcaption>\n",
    " </figure>\n",
    "</div>\n",
    "ResNet gives us the opportunity to train deeper networks, by adding residual blocks to the network within them, the information will not be transformed. The identity mapping propagates the information from on layer to the other, by just skipping the layers. This layers also called skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator: X -> Y\n",
    "## n_resnet is the number of resnet blocks\n",
    "g_model_XtoY = define.generator.res_net(image_shape, n_resnet= 9)\n",
    "# generator: Y -> X\n",
    "f_model_YtoX = define.generator.res_net(image_shape, n_resnet= 9)\n",
    "\n",
    "print(g_model_XtoY.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ResNet model consists of 14 Hidden Layers, 9 of them have skip connections. To make the training process more stable instance normalization were used. The activation function between the layers is ReLU and the last activation is tanh to produce values betwenn -1 und 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator LSGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As proposed in the CycleGAN paper, we will use the LSGAN model as the Discriminator for the generated pictures. LSGAN uses also the PatchGAN archticture, with the difference that instead of the sigmoider cross entropy the mean squared error is introduced. We chose a 1-0 Enconding scheme for real and fake pictures and optain:\n",
    "$$ L^{(D)}_{\\text{LSGAN}}(D, G) = \\frac{1}{N}\\sum_{i=1}^N(D(y) - 1)^2 + D(G(x))^2 $$\n",
    "$$ L^{(G)}_{\\text{LSGAN}}(D, G) = \\frac{1}{N}\\sum_{i=1}^N(D(G(x)) - 1)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator: X -> [real/fake]\n",
    "d_model_X = define.discriminator.lsgan(image_shape)\n",
    "# discriminator: Y -> [real/fake]\n",
    "d_model_Y = define.discriminator.lsgan(image_shape)\n",
    "print(d_model_Y.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CycleGAN we will train two adversial_models. Let $A_{G}$ and $A_{D}$ the models, then tying everything together leads to \n",
    "$$ A_{G} = \\underset{G}{\\arg \\min} \\left[ \\lambda_{1} \\cdot L^{(G)}_{\\text{LSGAN}}(D, G) + \\lambda_{2} \\cdot L_{\\text{L1}}(G) + \\lambda_{3} \\cdot  L_{\\text{cycle}}(G,F)  \\right] $$\n",
    "$$ A_{F} = \\underset{F}{\\arg \\min} \\left[ \\lambda_{1} \\cdot L^{(F)}_{\\text{LSGAN}}(D, F) + \\lambda_{2} \\cdot L_{\\text{L1}}(F) + \\lambda_{3} \\cdot  L_{\\text{cycle}}(G,F)  \\right] $$\n",
    "$$\\text{with} \\space \\space L_{\\text{cycle}} = \\frac{1}{NM} \\sum_{i,j} \\left|F(G(x))_{i,j} - x_{i,j}\\right| + \\left|G(F(y))_{i,j} - y_{i,j}\\right| $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this approach is just an extension to the pix2pix model. We see that the $L_{cycle}$ approach is heavily influenced by the $L_{1}$-loss. Just that we now try to make the reconstruction consistent. For this work $\\lambda_{1} = 1$, $\\lambda_{2}$ = 5  and $\\lambda_{3}$ = 10 were chosen. With chosing a high $\\lambda_{3}$ cycle consistancy have a high influence on the loss function. With $\\lambda_{2}$ we can make sure, that the structure of the picture will be preserved, as this is crucial for style transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversial: X -> Y -> [real/fake, X]\n",
    "a_model_XtoY = define.adversial_model.cycle_gan(g_model_XtoY, d_model_Y, f_model_YtoX, image_shape)\n",
    "# adversial: Y -> X -> [real/fake, Y]\n",
    "a_model_YtoX = define.adversial_model.cycle_gan(f_model_YtoX, d_model_X, g_model_XtoY, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "<div style=\"text-align: center;\">\n",
    "<figure>\n",
    "  <img src=\"pictures/cycle_gan_training.png\" alt=\"Bildbeschreibung\"  width=\"600\">\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some say a picture tells more then 1000 words. With respect to this flow chart we would argue that. Training a CycleGAN Network is very complicated but we will try to go to the process. So as we can see, the two Networks translate from $X \\rightarrow Y$ and from $Y \\rightarrow X$. Generator $G$ and $F$ generate fake images, which are stored into Pool A and B for computing the cycle-consistency-loss. Instead of activating the output the LSGAN Loss function takes the Output directly. The weights of the Generator and the Discriminator will be adjusted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan.cycle_gan(d_model_X, d_model_Y, g_model_XtoY, f_model_YtoX, a_model_XtoY, a_model_YtoX, dataset, n_epochs= 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with the models\n",
    "\n",
    "At this point the classes become handy, because we know can train the models with different discriminators and parameters. Just try it out.\n",
    "You can uncomment and comment the desired architecture, by playing around with the lambda-values you can control how important regularization becomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pix2Pix\n",
    "#g_model = define.generator.u_net(image_shape)\n",
    "g_model = define.generator.res_net(image_shape, n_resnet= 5)\n",
    "#d_model = define.discriminator.patchgan(image_shape)\n",
    "d_model = define.discriminator.lsgan(image_shape)\n",
    "lambda_1 = 1 #control the influence of the adversial loss; default = 1\n",
    "lambda_2 = 100 #control the influence of the L_1 pix loss; default = 100\n",
    "n_epochs = 100\n",
    "gan_model = define.adversial_model.pix2pix(g_model, d_model, image_shape,lambda_1,lambda_2)\n",
    "train_gan.pix2pix(d_model, g_model, gan_model,dataset=dataset, n_epochs= n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CycleGAN\n",
    "\n",
    "# generator: X -> Y\n",
    "## n_resnet is the number of resnet blocks\n",
    "g_model_XtoY = define.generator.res_net(image_shape, n_resnet= 5)\n",
    "# generator: Y -> X\n",
    "f_model_YtoX = define.generator.res_net(image_shape, n_resnet= 5)\n",
    "\n",
    "# generator: X -> Y\n",
    "## n_resnet is the number of resnet blocks\n",
    "#g_model_XtoY = define.generator.u_net(image_shape)\n",
    "# generator: Y -> X\n",
    "#f_model_YtoX = define.generator.u_net(image_shape)\n",
    "\n",
    "# discriminator: X -> [real/fake]\n",
    "d_model_X = define.discriminator.lsgan(image_shape)\n",
    "# discriminator: Y -> [real/fake]\n",
    "d_model_Y = define.discriminator.lsgan(image_shape)\n",
    "\n",
    "# discriminator: X -> [real/fake]\n",
    "#d_model_X = define.discriminator.patchgan(image_shape)\n",
    "# discriminator: Y -> [real/fake]\n",
    "#d_model_Y = define.discriminator.patchgan(image_shape)\n",
    "lambda_1 = 1 #control the influence of the adversial loss; default = 1\n",
    "lambda_2 = 5 #control the influence of the L_1 pix loss; default = 5\n",
    "lambda_3 = 10 #control the influence of the cycle consistency loss; default = 10\n",
    "n_epochs = 100\n",
    "# adversial: X -> Y -> [real/fake, X]\n",
    "a_model_XtoY = define.adversial_model.cycle_gan(g_model_XtoY, d_model_Y, f_model_YtoX, image_shape,lambda_1,lambda_2, lambda_3)\n",
    "# adversial: Y -> X -> [real/fake, Y]\n",
    "a_model_YtoX = define.adversial_model.cycle_gan(f_model_YtoX, d_model_X, g_model_XtoY, image_shape,lambda_1,lambda_2, lambda_3)\n",
    "train_gan.cycle_gan(d_model_X, d_model_Y, g_model_XtoY, f_model_YtoX, a_model_XtoY, a_model_YtoX, dataset, n_epochs= n_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
