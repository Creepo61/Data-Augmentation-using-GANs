{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Data Augmentation using GANs </center></h1>\n",
    "\n",
    "In this notebook we will introduce the Pix2Pix and CycleGAN training process on data created from carla simulator. In this repository you will find a smaller dataset for demonstration purposes, you can upload your own dataset for training purposes, but make sure that the dimensions fit. Our solution is based on the tensorflow library so make sure you have installed it. PIL is used for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to import the dataset. The Dataset consinsts of 60 Default and 60 rainy pictures. The Dimensions are height 255 and length 255 Pixels. They values are normalized to [-1,1] which is a standard procedure in image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    # load the compressed arrays\n",
    "    data = load(filename)\n",
    "    # unpack the arrays\n",
    "    X1, X2 = data['arr_0'], data['arr_1']\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X1 = (X1 - 127.5) / 127.5\n",
    "    X2 = (X2 - 127.5) / 127.5\n",
    "    return [X1, X2]\n",
    "\n",
    "###real data\n",
    "data = load(\"dataset_small.npz\")\n",
    "dataA, dataB = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', dataA.shape, dataB.shape)\n",
    "###normalized data\n",
    "dataset = load_dataset(\"dataset_small.npz\")\n",
    "# Größe der Bilder festlegen\n",
    "height = dataA.shape[1] \n",
    "width = dataA.shape[2] // 5\n",
    "\n",
    "# Vergrößere die verfügbare Fläche, um sicherzustellen, dass die Bilder quadratisch bleiben\n",
    "new_width = width * 5 # Zum Beispiel, um die Breite auf das Fünffache zu erhöhen\n",
    "\n",
    "# plot source images\n",
    "n_samples = 5\n",
    "random_index =  [random.randint(1,60) for _ in range(n_samples)]\n",
    "for idx, i in enumerate(random_index, 1):\n",
    "    pyplot.subplot(2, n_samples, idx)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(dataA[i].astype('uint8'), extent=[0, new_width, 0, height])\n",
    "\n",
    "# Plot target images\n",
    "for idx, i in enumerate(random_index, 1):\n",
    "    pyplot.subplot(2, n_samples, n_samples + idx)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(dataB[i].astype('uint8'), extent=[0, new_width, 0, height])\n",
    "\n",
    "pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix and CycleGAN Networks\n",
    "\n",
    "This code defines a class `define` that encapsulates the architecture of both Pix2Pix and CycleGAN networks. The class is further divided into subclasses for discriminators (`discriminator`), generators (`generator`), and composite models (`composite_model`). This code provides a modular and reusable implementation of these architectures for image-to-image translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class define():\n",
    "    class discriminator:\n",
    "      def patchgan(image_shape):   \n",
    "          # weight initialization\n",
    "            init = RandomNormal(stddev=0.02)\n",
    "            # source image input\n",
    "            in_src_image = Input(shape=image_shape)\n",
    "            # target image input\n",
    "            ### patchgan will take the binary loss\n",
    "            in_target_image = Input(shape=image_shape)\n",
    "            # concatenate images channel-wise\n",
    "            merged = Concatenate()([in_src_image, in_target_image])\n",
    "            # C64\n",
    "            d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # C128\n",
    "            d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # C256\n",
    "            d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # C512\n",
    "            d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # second last output layer\n",
    "            d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            d = BatchNormalization()(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # patch output\n",
    "            d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            patch_out = Activation('sigmoid')(d)\n",
    "            # define model\n",
    "            model = Model([in_src_image, in_target_image], patch_out)\n",
    "            # compile model\n",
    "            opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "            model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "            return model\n",
    "      \n",
    "      def ls_gan(image_shape):\n",
    "          # weight initialization\n",
    "            init = RandomNormal(stddev=0.02)\n",
    "            # source image input\n",
    "            in_image = Input(shape=image_shape)\n",
    "            ##cnn will take the mse\n",
    "            # C64\n",
    "            d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # C128\n",
    "            d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # C256\n",
    "            d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # C512\n",
    "            d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # second last output layer\n",
    "            d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            d = InstanceNormalization(axis=-1)(d)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            # patch output\n",
    "            patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "            # define model\n",
    "            model = Model(in_image, patch_out)\n",
    "            # compile model\n",
    "            model.compile(loss='mse', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss_weights=[0.5])\n",
    "            return model\n",
    "    \n",
    "    class generator:\n",
    "      def __define_encoder_block__(layer_in, n_filters, batchnorm=True):\n",
    "          # weight initialization\n",
    "          init = RandomNormal(stddev=0.02)\n",
    "          # add downsampling layer\n",
    "          g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same',\n",
    "          kernel_initializer=init)(layer_in)\n",
    "          # conditionally add batch normalization\n",
    "          if batchnorm:\n",
    "              g = BatchNormalization()(g, training=True)\n",
    "          # leaky relu activation\n",
    "          g = LeakyReLU(alpha=0.2)(g)\n",
    "          return g\n",
    "      def __decoder_block__(layer_in, skip_in, n_filters, dropout=True):\n",
    "          # weight initialization\n",
    "          init = RandomNormal(stddev=0.02)\n",
    "          # add upsampling layer\n",
    "          g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same',\n",
    "          kernel_initializer=init)(layer_in)\n",
    "          # add batch normalization\n",
    "          g = BatchNormalization()(g, training=True)\n",
    "          # conditionally add dropout\n",
    "          if dropout:\n",
    "              g = Dropout(0.5)(g, training=True)\n",
    "          # merge with skip connection\n",
    "          g = Concatenate()([g, skip_in])\n",
    "          # relu activation\n",
    "          g = Activation('relu')(g)\n",
    "          return g\n",
    "      def _resnet_block(n_filters, input_layer):\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # first layer convolutional layer\n",
    "        g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        # second convolutional layer\n",
    "        g = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        # concatenate merge channel-wise with input layer\n",
    "        g = Concatenate()([g, input_layer])\n",
    "        return g\n",
    "      \n",
    "      def u_net(image_shape):\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # image input\n",
    "        in_image = Input(shape=image_shape)\n",
    "        # encoder model\n",
    "        e1 = define.generator.__define_encoder_block__(in_image, 64, batchnorm=False)\n",
    "        e2 = define.generator.__define_encoder_block__(e1, 128)\n",
    "        e3 = define.generator.__define_encoder_block__(e2, 256)\n",
    "        e4 = define.generator.__define_encoder_block__(e3, 512)\n",
    "        e5 = define.generator.__define_encoder_block__(e4, 512)\n",
    "        e6 = define.generator.__define_encoder_block__(e5, 512)\n",
    "        e7 = define.generator.__define_encoder_block__(e6, 512)\n",
    "        # bottleneck, no batch norm and relu\n",
    "        b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "        b = Activation('relu')(b)\n",
    "        # decoder model\n",
    "        d1 = define.generator.__decoder_block__(b, e7, 512)\n",
    "        d2 = define.generator.__decoder_block__(d1, e6, 512)\n",
    "        d3 = define.generator.__decoder_block__(d2, e5, 512)\n",
    "        d4 = define.generator.__decoder_block__(d3, e4, 512, dropout=False)\n",
    "        d5 = define.generator.__decoder_block__(d4, e3, 256, dropout=False)\n",
    "        d6 = define.generator.__decoder_block__(d5, e2, 128, dropout=False)\n",
    "        d7 = define.generator.__decoder_block__(d6, e1, 64, dropout=False)\n",
    "        # output\n",
    "        g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "        out_image = Activation('tanh')(g)\n",
    "        # define model\n",
    "        model = Model(in_image, out_image)\n",
    "        return model\n",
    "      def res_net(image_shape, n_resnet=9):\n",
    "          # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # image input\n",
    "        in_image = Input(shape=image_shape)\n",
    "        # c7s1-64\n",
    "        g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        # d128\n",
    "        g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        # d256\n",
    "        g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        # R256\n",
    "        for _ in range(n_resnet):\n",
    "          g = define.generator._resnet_block(256, g)\n",
    "        # u128\n",
    "        g = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        # u64\n",
    "        g = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        g = Activation('relu')(g)\n",
    "        # c7s1-3\n",
    "        g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n",
    "        g = InstanceNormalization(axis=-1)(g)\n",
    "        out_image = Activation('tanh')(g)\n",
    "        # define model\n",
    "        model = Model(in_image, out_image)\n",
    "        return model\n",
    "    class composite_model:\n",
    "       def cycle_gan(g_model_1, d_model, g_model_2, image_shape):\n",
    "        g_model_1.trainable = True\n",
    "        d_model.trainable = False\n",
    "        g_model_2.trainable = False\n",
    "        input_gen = Input(shape=image_shape)\n",
    "        gen1_out = g_model_1(input_gen)\n",
    "        output_d = d_model(gen1_out)\n",
    "        input_id = Input(shape = image_shape)\n",
    "        output_id = g_model_1(input_id)\n",
    "        output_f = g_model_2(gen1_out)\n",
    "        gen2_out = g_model_2(input_id)\n",
    "        output_b = g_model_1(gen2_out)\n",
    "        model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
    "        return model\n",
    "       def pix2pix(g_model, d_model, image_shape):\n",
    "        # make weights in the discriminator not trainable,\n",
    "        # since Batchnormalization must not be \"learned\"\n",
    "        for layer in d_model.layers:\n",
    "            if not isinstance(layer, BatchNormalization):\n",
    "                layer.trainable = False\n",
    "        # define the source image\n",
    "        in_src = Input(shape=image_shape)\n",
    "        # connect the source image to the generator input\n",
    "        gen_out = g_model(in_src)\n",
    "        # connect the source input and generator output to the discriminator input\n",
    "        dis_out = d_model([in_src, gen_out])\n",
    "        # src image as input, generated image and classification output\n",
    "        model = Model(in_src, [dis_out, gen_out])\n",
    "        # compile model\n",
    "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "        return model\n",
    "          \n",
    "\n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summaries and Comparisons\n",
    "\n",
    "This code initializes and summarizes several image-to-image translation models using the above defined architectures from the `define` class. The summaries are printed and compared using the `tabulate` library for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = dataset[0].shape[1:]\n",
    "res = define.generator.res_net(image_shape=image_shape)\n",
    "ls_gan = define.discriminator.ls_gan(image_shape=image_shape)\n",
    "patch_gan = define.discriminator.patchgan(image_shape=image_shape)\n",
    "unet = define.generator.u_net(image_shape=image_shape)\n",
    "summary_ls_gan = []\n",
    "ls_gan.summary(print_fn=lambda x: summary_ls_gan.append(x), line_length=80)\n",
    "summary_ls_gan = \"\\n\".join(summary_ls_gan)\n",
    "summary_res = []\n",
    "res.summary(print_fn=lambda x: summary_res.append(x), line_length=80)\n",
    "summary_res = \"\\n\".join(summary_res)\n",
    "summary_patch_gan = []\n",
    "patch_gan.summary(print_fn=lambda x: summary_patch_gan.append(x), line_length=80)\n",
    "summary_patch_gan = \"\\n\".join(summary_patch_gan)\n",
    "summary_unet = []\n",
    "unet.summary(print_fn=lambda x: summary_unet.append(x), line_length=80)\n",
    "summary_unet = \"\\n\".join(summary_unet)\n",
    "table = tabulate([[summary_patch_gan, summary_unet]], headers=[\"LSGAN\", \"ResNet\"], tablefmt='plain', colalign=(\"left\", \"left\"))\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picture Generation Utilities\n",
    "\n",
    "This code defines a utility class `generate_pictures` with two sub-classes (`real` and `fake`) responsible for generating real and fake instances for training the Pix2Pix and CycleGAN models.\n",
    "\n",
    "By using separate methods for real and fake instance generation, we provide flexibility and clarity for users working with either Pix2Pix or CycleGAN models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate_pictures():\n",
    "    class real():\n",
    "        def cycle_gan(dataset, n_samples, patch_shape):\n",
    "            #choose random pictures inside the dataset\n",
    "            ix = randint(0, dataset.shape[0], n_samples)\n",
    "            X = dataset[ix]\n",
    "            y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "            return X, y\n",
    "        def pix2pix(dataset, n_samples, patch_shape):\n",
    "            # unpack dataset\n",
    "            trainA, trainB = dataset\n",
    "            # choose random instances\n",
    "            ix = randint(0, trainA.shape[0], n_samples)\n",
    "            # retrieve selected images\n",
    "            X1, X2 = trainA[ix], trainB[ix]\n",
    "            # generate 'real' class labels (1)\n",
    "            y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "            return [X1, X2], y\n",
    "    class fake():\n",
    "        ###both methods do the same, but the user would be irritated if there would be just one option\n",
    "        def cycle_gan(g_model, dataset, patch_shape):\n",
    "            # generate fake instance\n",
    "            X = g_model.predict(dataset)\n",
    "            # create 'fake' class labels (0)\n",
    "            y = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "            return X, y\n",
    "        def pix2pix(g_model, samples, patch_shape):\n",
    "            # generate fake instance\n",
    "            X = g_model.predict(samples)\n",
    "            # create 'fake' class labels (0)\n",
    "            y = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "            return X, y\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Visualization Utilities\n",
    "\n",
    "The `performance` class provides methods to visualize and compare Pix2Pix and CycleGAN model performance during training. `pix2pix_loss` and `cycle_gan_loss` plot training losses, while `pix2pix_pictures` and `cycle_gan_pictures` display image translations at different training steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class performance():\n",
    "    def pix2pix_loss(d_loss1_list, d_loss2_list, g_loss_list):\n",
    "        steps = range(1, len(d_loss1_list) + 1)\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.title('Training Losses over time - Step %d' %max(steps))\n",
    "        plt.plot(steps, d_loss1_list, label='D Loss Real', color='blue')\n",
    "        plt.plot(steps, d_loss2_list, label='D Loss Fake', color='green')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(steps, g_loss_list, label='G Loss', color='orange')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        # Rückgabe des Plot-Objekts\n",
    "        plt.show()\n",
    "    def pix2pix_pictures(step, g_model, dataset, n_samples=5):\n",
    "        [X_realA, X_realB], _ = generate_pictures.real.pix2pix(dataset, n_samples, 1)\n",
    "        # generate a batch of fake samples\n",
    "        X_fakeB, _ = generate_pictures.fake.pix2pix(g_model, X_realA, 1)\n",
    "        # scale all pixels from [-1,1] to [0,1]\n",
    "        X_realA = (X_realA + 1) / 2.0\n",
    "        X_realB = (X_realB + 1) / 2.0\n",
    "        X_fakeB = (X_fakeB + 1) / 2.0\n",
    "        # plot real source images\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, 1 + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_realA[i])\n",
    "        # plot generated target image\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, 1 + n_samples + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_fakeB[i])\n",
    "        # plot real target image\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_realB[i])\n",
    "        plt.figtext(0.05, 0.95, \"Step %d\" % step, fontsize=12, ha='center')\n",
    "        plt.show()\n",
    "            \n",
    "\n",
    "    def cycle_gan_pictures(step, g_model, trainX, name, n_samples=5):\n",
    "        # select a sample of input images\n",
    "        X_in, _ = generate_pictures.real.cycle_gan(trainX, n_samples, 0)\n",
    "        # generate translated images\n",
    "        X_out, _ = generate_pictures.fake.cycle_gan(g_model, X_in, 0)\n",
    "        # scale all pixels from [-1,1] to [0,1]\n",
    "        X_in = (X_in + 1) / 2.0\n",
    "        X_out = (X_out + 1) / 2.0\n",
    "        # plot real images\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(2, n_samples, 1 + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_in[i])\n",
    "        # plot translated image\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(2, n_samples, 1 + n_samples + i)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(X_out[i])\n",
    "        plt.figtext(0.05, 0.95, \"%d. Epoche\" % step, fontsize=12, ha='left')\n",
    "        plt.show()\n",
    "    def cycle_gan_loss(dG_loss1_list, dG_loss2_list , g_loss_list, f_loss_list, dF_loss1_list ,dF_loss2_list):\n",
    "        steps = range(1, len(dG_loss1_list) + 1)\n",
    "        plt.subplot(2,4,1)\n",
    "        plt.title('Training Losses over time - Step %d' %max(steps))\n",
    "        plt.plot(steps, dG_loss1_list, label='D_G Loss Real', color='blue')\n",
    "        plt.plot(steps, dG_loss2_list, label='D_G Loss Fake', color='green')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.subplot(2,4,2)\n",
    "        plt.plot(steps, g_loss_list, label='G Loss', color='orange')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.subplot(2,4,3)\n",
    "        plt.title('Training Losses over time - Step %d' %max(steps))\n",
    "        plt.plot(steps, dF_loss1_list, label='D_F Loss Real', color='blue')\n",
    "        plt.plot(steps, dF_loss2_list, label='D_F Loss Fake', color='green')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.subplot(2,4,4)\n",
    "        plt.plot(steps, f_loss_list, label='F Loss', color='orange')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training Utilities\n",
    "\n",
    "The `train_gan` class puts together the training methods for the Pix2Pix and CycleGAN models. The `pix2pix` method trains a Pix2Pix model, updating discriminator and generator losses. The `cycle_gan` method trains a CycleGAN model, updating discriminator, generator, and cycle consistency losses. Both methods provide visualizations of the training progress using the `performance` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_gan():\n",
    "    def __update_image_pool__(pool, images, max_size=50):\n",
    "        selected = list()\n",
    "        for image in images:\n",
    "            if len(pool) < max_size:\n",
    "                # stock the pool\n",
    "                pool.append(image)\n",
    "                selected.append(image)\n",
    "            elif random() < 0.5:\n",
    "                # use image, but don't add it to the pool\n",
    "                selected.append(image)\n",
    "            else:\n",
    "                # replace an existing image and use replaced image\n",
    "                ix = randint(0, len(pool))\n",
    "                selected.append(pool[ix])\n",
    "                pool[ix] = image\n",
    "        return asarray(selected)\n",
    "    def pix2pix(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n",
    "        # determine the output square shape of the discriminator\n",
    "        n_patch = d_model.output_shape[1]\n",
    "        # calculate the number of batches per training epoch\n",
    "        bat_per_epo = int(len(dataset[0]) / n_batch)\n",
    "        # calculate the number of training iterations\n",
    "        n_steps = bat_per_epo * n_epochs\n",
    "        # manually enumerate epochs\n",
    "        d_loss1_list = []\n",
    "        d_loss2_list = []\n",
    "        g_loss_list = []\n",
    "        for i in range(n_steps):\n",
    "            # select a batch of real samples\n",
    "            [X_realA, X_realB], y_real = generate_pictures.real.pix2pix(dataset, n_batch, n_patch)\n",
    "            # generate a batch of fake samples\n",
    "            X_fakeB, y_fake = generate_pictures.fake.pix2pix(g_model, X_realA, n_patch)\n",
    "            # update discriminator for real samples\n",
    "            d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "            # update discriminator for generated samples\n",
    "            d_loss1_list.append(d_loss1)\n",
    "            d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "            d_loss2_list.append(d_loss2)\n",
    "            # update the generator\n",
    "            g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "            g_loss_list.append(g_loss)\n",
    "            # summarize performance\n",
    "            clear_output(wait=True)\n",
    "            performance.pix2pix_pictures(i, g_model, dataset, n_samples=5)\n",
    "            performance.pix2pix_loss(d_loss1_list, d_loss2_list,g_loss_list)\n",
    "   \n",
    "\n",
    "    def cycle_gan(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n",
    "        # define properties of the training run\n",
    "        n_epochs, n_batch, = 100, 1\n",
    "        # determine the output square shape of the discriminator\n",
    "        n_patch = d_model_A.output_shape[1]\n",
    "        # unpack dataset\n",
    "        trainA, trainB = dataset\n",
    "        # prepare image pool for fakes\n",
    "        poolA, poolB = list(), list()\n",
    "        # calculate the number of batches per training epoch\n",
    "        bat_per_epo = int(len(trainA) / n_batch)\n",
    "        # calculate the number of training iterations\n",
    "        n_steps = bat_per_epo * n_epochs\n",
    "        # manually enumerate epochs\n",
    "        dG_loss1_list = []\n",
    "        dG_loss2_list = []\n",
    "        g_loss_list = []\n",
    "        f_loss_list = []\n",
    "        dF_loss1_list = []\n",
    "        dF_loss2_list = []\n",
    "        for i in range(n_steps):\n",
    "            # select a batch of real samples\n",
    "            X_realA, y_realA = generate_pictures.real.cycle_gan(trainA, n_batch, n_patch)\n",
    "            X_realB, y_realB = generate_pictures.real.cycle_gan(trainB, n_batch, n_patch)\n",
    "            # generate a batch of fake samples\n",
    "            X_fakeA, y_fakeA = generate_pictures.fake.cycle_gan(g_model_BtoA, X_realB, n_patch)\n",
    "            X_fakeB, y_fakeB = generate_pictures.fake.cycle_gan(g_model_AtoB, X_realA, n_patch)\n",
    "            # update fakes from pool\n",
    "            X_fakeA = train_gan.__update_image_pool__(poolA, X_fakeA)\n",
    "            X_fakeB = train_gan.__update_image_pool__(poolB, X_fakeB)\n",
    "            # update generator B->A via adversarial and cycle loss\n",
    "            g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
    "            f_loss_list.append(g_loss2)\n",
    "            # update discriminator for A -> [real/fake]\n",
    "            dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
    "            dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
    "            dG_loss1_list.append(dA_loss1)\n",
    "            dG_loss2_list.append(dA_loss2)\n",
    "            # update generator A->B via adversarial and cycle loss\n",
    "            g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
    "            g_loss_list.append(g_loss1)\n",
    "            # update discriminator for B -> [real/fake]\n",
    "            dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
    "            dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
    "            dF_loss1_list.append(dB_loss1)\n",
    "            dF_loss2_list.append(dB_loss2)\n",
    "            # summarize performance\n",
    "            performance.cycle_gan_pictures(i, g_model_AtoB, trainA, 'AtoB')\n",
    "            \n",
    "            performance.cycle_gan_loss(dG_loss1_list, dG_loss2_list , g_loss_list, f_loss_list, dF_loss1_list ,dF_loss2_list)\n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix Live Training\n",
    "\n",
    "In the following cells, the Pix2Pix network will undergo live training, and the training results, including loss plots and image translations, will be displayed below. The training progress will be visualized using the defined `performance` utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_pix2pix = tabulate([[summary_patch_gan, summary_unet]], headers=[\"PatchGan\", \"Unet\"], tablefmt='plain', colalign=(\"left\", \"left\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_model = define.discriminator.patchgan(image_shape)\n",
    "g_model = define.generator.u_net(image_shape)\n",
    "# define the composite model\n",
    "gan_model = define.composite_model.pix2pix(g_model, d_model, image_shape)\n",
    "# train model\n",
    "train_gan.pix2pix(d_model, g_model, gan_model,dataset=dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### training cyclegan\n",
    "\n",
    "# generator: A -> B\n",
    "###Anzahl der ResNet-Blöcke als Hyperparamater\n",
    "g_model_AtoB = define.generator.res_net(image_shape, n_resnet=9)\n",
    "# generator: B -> A\n",
    "g_model_BtoA = define.generator.res_net(image_shape, n_resnet=9)\n",
    "# discriminator: A -> [real/fake]\n",
    "d_model_A = define.discriminator.ls_gan(image_shape)\n",
    "# discriminator: B -> [real/fake]\n",
    "d_model_B = define.discriminator.ls_gan(image_shape)\n",
    "# composite: A -> B -> [real/fake, A]\n",
    "c_model_AtoB = define.composite_model.cycle_gan(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n",
    "# composite: B -> A -> [real/fake, B]\n",
    "c_model_BtoA = define.composite_model.cycle_gan(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n",
    "# train models\n",
    "train_gan.cycle_gan(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
